{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def dont_dropna(df_areas):\n",
    "    df_moves = df_areas[['effdate', 'fips']]\n",
    "    df_moves = df_moves.rename(\n",
    "        columns={'effdate': 'date_left', 'fips': 'dest_fips'}\n",
    "    )\n",
    "    df_moves[['date_arrived', 'orig_fips']] = (\n",
    "        df_moves.groupby(df_moves.index)[['date_left', 'dest_fips']]\n",
    "            .shift()\n",
    "    )\n",
    "    return df_moves\n",
    "\n",
    "def preshift_dropna(df_areas):\n",
    "    # dropna fips before shifting\n",
    "    df_moves = df_areas[['effdate', 'fips']].dropna()\n",
    "    df_moves = df_moves.rename(\n",
    "        columns={'effdate': 'date_left', 'fips': 'dest_fips'}\n",
    "    )\n",
    "    df_moves[['date_arrived', 'orig_fips']] = (\n",
    "        df_moves.groupby(df_moves.index)[['date_left', 'dest_fips']]\n",
    "            .shift()\n",
    "    )\n",
    "    return df_moves\n",
    "\n",
    "def postshift_dropna(df_areas):\n",
    "    # dropna after shifting\n",
    "    df_moves = df_areas[['effdate', 'fips']]\n",
    "    df_moves = df_moves.rename(\n",
    "        columns={'effdate': 'date_left', 'fips': 'dest_fips'}\n",
    "    )\n",
    "    df_moves[['date_arrived', 'orig_fips']] = (\n",
    "        df_moves.groupby(df_moves.index)[['date_left', 'dest_fips']]\n",
    "            .shift()\n",
    "    )\n",
    "    df_moves = df_moves.dropna()\n",
    "    return df_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Options ###\n",
    "lines_per_chunk = 1_250_000\n",
    "z4types = ('H', 'S', 'R')\n",
    "create_moves = dont_dropna\n",
    "county_codes = (\"037\", \"059\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract all moves ###\n",
    "\n",
    "# Settings\n",
    "usecols_all_states = ['z4type', 'effdate']\n",
    "for i in range(2, 11):\n",
    "    usecols_all_states.append(f'z4type{i}')\n",
    "    usecols_all_states.append(f'effdate{i}')\n",
    "for i in range(1, 11):\n",
    "    usecols_all_states.append(f'fips{i}')\n",
    "\n",
    "dtype_all_states = {\n",
    "    'z4type': 'category',\n",
    "    'effdate': 'float',\n",
    "    'fips1': 'object',\n",
    "}\n",
    "for i in range(2, 11):\n",
    "    dtype_all_states[f'z4type{i}'] = 'category'\n",
    "    dtype_all_states[f'effdate{i}'] = 'float'\n",
    "    dtype_all_states[f'fips{i}'] = 'object'\n",
    "\n",
    "list_dict_col_names = [\n",
    "    {\n",
    "        'z4type': 'z4type',\n",
    "        'effdate': 'effdate',\n",
    "        'fips1': 'fips',\n",
    "    }\n",
    "]\n",
    "for i in range(2, 11):\n",
    "    list_dict_col_names.append(\n",
    "        {\n",
    "            f'z4type{i}': 'z4type',\n",
    "            f'effdate{i}': 'effdate',\n",
    "            f'fips{i}': 'fips',\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Load all_states\n",
    "it_all_states = pd.read_csv(\n",
    "    \"data/all_states.csv\", \n",
    "    usecols=usecols_all_states,\n",
    "    dtype=dtype_all_states,\n",
    "    chunksize=lines_per_chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create status file\n",
    "try:\n",
    "    f = open(f\"inter_status-{lines_per_chunk}.txt\")\n",
    "except FileNotFoundError:\n",
    "    f = open(f\"inter_status-{lines_per_chunk}.txt\", 'a')\n",
    "    finished_chunks = set()\n",
    "else:\n",
    "    finished_chunks = {int(line) for line in f.readlines()}\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# Create chunk folders\n",
    "Path(f\"data/inter/moves-{lines_per_chunk}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "Path(f\"data/inter/08_17_moves-{lines_per_chunk}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "Path(f\"data/inter/03_12_moves-{lines_per_chunk}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "Path(f\"data/inter/03_17_moves-{lines_per_chunk}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all states\n",
    "\n",
    "def process_chunk(index, chunk):\n",
    "    if index in finished_chunks:\n",
    "        return f\"Chunk {index} done\" \n",
    "\n",
    "    # Split rows into individual areas\n",
    "    list_df_all_areas = [\n",
    "        chunk[list_dict_col_names[i].keys()].rename(\n",
    "            columns=list_dict_col_names[i]\n",
    "        ) for i in range(10)\n",
    "    ]\n",
    "\n",
    "    # Recombine all areas + sort by effdate\n",
    "    df_all_areas = (\n",
    "        pd.concat(list_df_all_areas)\n",
    "            .dropna(subset=['effdate'])\n",
    "            .sort_values('effdate', kind='stable')\n",
    "    )\n",
    "\n",
    "    # All effdates that do not have a z4type in z4types\n",
    "    bi_all_dropped = ~(\n",
    "        df_all_areas['z4type'].isin(z4types)\n",
    "            .groupby([df_all_areas.index, df_all_areas['effdate']])\n",
    "            .transform('any')\n",
    "    )\n",
    "\n",
    "    # Change values so selected effdates are not removed\n",
    "    df_all_areas.loc[bi_all_dropped, 'z4type'] = 'empty'\n",
    "    df_all_areas.loc[bi_all_dropped, 'fips'] = \"\"\n",
    "\n",
    "    # Filter by Zip+4 type\n",
    "    z4types_mask = (*z4types, 'empty')\n",
    "\n",
    "    df_filtered_areas = df_all_areas[df_all_areas['z4type'].isin(z4types_mask)]\n",
    "\n",
    "    # Choose leftmost fips of each effdate\n",
    "    df_filtered_areas = (\n",
    "        df_filtered_areas.groupby([df_filtered_areas.index, 'effdate']).first()\n",
    "    )\n",
    "    df_filtered_areas = df_filtered_areas.reset_index('effdate')\n",
    "\n",
    "    # Link previous & next areas as moves\n",
    "    df_all_moves = create_moves(df_filtered_areas)\n",
    "\n",
    "    # Filter by county code\n",
    "    df_filtered_moves = df_all_moves[\n",
    "        df_all_moves['orig_fips'].str[2:5].isin(county_codes) |\n",
    "            df_all_moves['dest_fips'].str[2:5].isin(county_codes)\n",
    "    ]\n",
    "\n",
    "    df_filtered_moves.to_csv(\n",
    "        f\"data/inter/moves-{lines_per_chunk}/{index}.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    # Separate moves\n",
    "    extract = (\n",
    "        lambda arrived_start, arrived_stop, left_start, left_stop:\n",
    "        df_filtered_moves[\n",
    "            df_filtered_moves['date_arrived'].between(\n",
    "                arrived_start, arrived_stop, inclusive='left'\n",
    "            )\n",
    "            & df_filtered_moves['date_left'].between(\n",
    "                left_start, left_stop, inclusive='left'\n",
    "            )\n",
    "        ][['orig_fips', 'dest_fips']]\n",
    "    )\n",
    "\n",
    "    df_08_17_moves = extract(200801, 201301, 201301, 201801)\n",
    "    df_03_12_moves = extract(200301, 200801, 200801, 201301)\n",
    "    df_03_17_moves = extract(200301, 200801, 201301, 201801)\n",
    "\n",
    "    # Write to files\n",
    "    df_08_17_moves.to_csv(\n",
    "        f\"data/inter/08_17_moves-{lines_per_chunk}/{index}.csv\", index=False\n",
    "    )\n",
    "    df_03_12_moves.to_csv(\n",
    "        f\"data/inter/03_12_moves-{lines_per_chunk}/{index}.csv\", index=False\n",
    "    )\n",
    "    df_03_17_moves.to_csv(\n",
    "        f\"data/inter/03_17_moves-{lines_per_chunk}/{index}.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # Update status\n",
    "    with open(f\"inter_status-{lines_per_chunk}.txt\", 'a') as status_file:\n",
    "        status_file.write(f\"{index}\\n\")\n",
    "\n",
    "    return f\"Chunk {index} done\"\n",
    "\n",
    "\n",
    "for index, chunk in enumerate(it_all_states):\n",
    "    process_chunk(index, chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load all moves ###\n",
    "\n",
    "def read_chunks(period):\n",
    "    # Settings\n",
    "    dtype_all_moves = {\n",
    "        'orig_fips': 'object',\n",
    "        'dest_fips': 'object',\n",
    "    }\n",
    "\n",
    "    # Read files\n",
    "    df_moves = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(chunk_file, dtype=dtype_all_moves)\n",
    "            for chunk_file in glob(\n",
    "                f\"data/inter/{period}-{lines_per_chunk}/*.csv\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df_moves\n",
    "\n",
    "\n",
    "df_08_17_moves = read_chunks(\"08_17_moves\")\n",
    "df_03_12_moves = read_chunks(\"03_12_moves\")\n",
    "df_03_17_moves = read_chunks(\"03_17_moves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data analysis ###\n",
    "\n",
    "def areas_stayed_count(df_moves):\n",
    "    se_count = df_moves.groupby('orig_fips').size()\n",
    "\n",
    "    se_count.index.name = 'fips'\n",
    "\n",
    "    return se_count\n",
    "\n",
    "se_08_17_count = areas_stayed_count(df_08_17_moves)\n",
    "se_03_12_count = areas_stayed_count(df_03_12_moves)\n",
    "se_03_17_count = areas_stayed_count(df_03_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_08_17_count.to_csv(f\"data/inter/08_17_areas_stayed.csv\")\n",
    "se_03_12_count.to_csv(f\"data/inter/03_12_areas_stayed.csv\")\n",
    "se_03_17_count.to_csv(f\"data/inter/03_17_areas_stayed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame(columns=['count'])\n",
    "\n",
    "df_summary.loc[\"08_17_total\"] = se_08_17_count.size\n",
    "df_summary.loc[\"03_12_total\"] = se_03_12_count.size\n",
    "df_summary.loc[\"03_17_total\"] = se_03_17_count.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv(f\"data/inter/summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infutor-research-jupyterlab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b170992dff1a2d7a7d34a8e195c66b869f5a2c2d367c7625363ea5e606f09977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
