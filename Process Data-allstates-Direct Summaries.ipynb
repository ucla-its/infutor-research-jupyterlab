{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Options ###\n",
    "z4types = ('H', 'S')\n",
    "county_codes = (\"037\", \"059\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract all moves ###\n",
    "\n",
    "# Load all_states\n",
    "df_all_states = dd.read_csv(\"data/all_states.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "list_dict_col_names = [\n",
    "    {\n",
    "        'z4type': 'z4type',\n",
    "        'effdate': 'effdate',\n",
    "        'fips1': 'fips',\n",
    "    }\n",
    "]\n",
    "for i in range(2, 11):\n",
    "    list_dict_col_names.append(\n",
    "        {\n",
    "            f'z4type{i}': 'z4type',\n",
    "            f'effdate{i}': 'effdate',\n",
    "            f'fips{i}': 'fips',\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Split rows into individual areas\n",
    "list_df_all_areas = [\n",
    "    df_all_states[list_dict_col_names[i].keys()].rename(\n",
    "        columns=list_dict_col_names[i]\n",
    "    ) for i in range(10)\n",
    "]\n",
    "\n",
    "# Reconstitute all areas + sort effdate\n",
    "df_all_areas = dd.concat(list_df_all_areas).sort_values('effdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_areas_dropna = df_all_areas.dropna(subset='effdate')\n",
    "\n",
    "# All effdates that do not have a z4type in z4types\n",
    "bi_all_dropped = ~(\n",
    "    df_all_areas_dropna['z4type'].isin(z4types)\n",
    "        .groupby([df_all_areas_dropna.index, df_all_areas_dropna['effdate']])\n",
    "        .transform('any')\n",
    ")\n",
    "\n",
    "# Change values so selected effdates are not removed\n",
    "df_all_areas_dropna.loc[bi_all_dropped, 'z4type'] = 'empty'\n",
    "df_all_areas_dropna.loc[bi_all_dropped, 'fips'] = \"\"\n",
    "\n",
    "# Filter by Zip+4 type\n",
    "z4types_mask = (*z4types, 'empty')\n",
    "\n",
    "df_filtered_areas = df_all_areas_dropna[\n",
    "    df_all_areas_dropna['z4type'].isin(z4types_mask)\n",
    "]\n",
    "\n",
    "# Choose leftmost fips of each effdate\n",
    "df_filtered_areas = (\n",
    "    df_filtered_areas.groupby([df_filtered_areas.index, 'effdate']).first()\n",
    ")\n",
    "df_filtered_areas = df_filtered_areas.reset_index('effdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link previous & next areas as moves\n",
    "df_all_moves = df_filtered_areas[['effdate', 'fips']]\n",
    "df_all_moves = df_all_moves.rename(columns={'fips': 'destfips'})\n",
    "df_all_moves['origfips'] = (\n",
    "    df_all_moves.groupby(df_all_moves.index)['destfips']\n",
    "    .shift(fill_value=\"first record\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by county code\n",
    "df_filtered_moves = df_all_moves[\n",
    "    df_all_moves['origfips'].str[2:5].isin(county_codes) |\n",
    "        df_all_moves['destfips'].str[2:5].isin(county_codes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate moves into periods\n",
    "\n",
    "def extract_period(start, stop):\n",
    "    return df_filtered_moves[\n",
    "        df_filtered_moves['effdate'].between(start, stop, inclusive='left')\n",
    "    ][['origfips', 'destfips']]\n",
    "\n",
    "df_03_07_moves = extract_period(200301, 200801)\n",
    "df_08_12_moves = extract_period(200801, 201301)\n",
    "df_13_17_moves = extract_period(201301, 201801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data analysis ###\n",
    "\n",
    "# Settings\n",
    "usecols_fips_tract = ['tractid_fips', 'gainers', 'losers']\n",
    "\n",
    "dtype_fips_tract = {\n",
    "    'tractid_fips': 'object',\n",
    "    'gainers': 'bool',\n",
    "    'losers': 'bool',\n",
    "}\n",
    "\n",
    "\n",
    "# Load fips_tracts_cats\n",
    "df_fips_tract = pd.read_csv(\n",
    "    \"fips_tracts_cats.csv\",\n",
    "    usecols=usecols_fips_tract,\n",
    "    dtype=dtype_fips_tract\n",
    ")\n",
    "\n",
    "# Get Series of gainers and losers\n",
    "se_gainers = df_fips_tract[df_fips_tract['gainers']]['tractid_fips']\n",
    "se_losers = df_fips_tract[df_fips_tract['losers']]['tractid_fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gain & loss totals\n",
    "\n",
    "def move_totals(df_moves):\n",
    "    se_gain_total = (\n",
    "        df_moves.groupby('destfips')\n",
    "            .size()\n",
    "            .reindex(se_gainers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    se_loss_total = (\n",
    "        df_moves.groupby('origfips')\n",
    "            .size()\n",
    "            .reindex(se_losers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    return se_gain_total, se_loss_total\n",
    "\n",
    "se_03_07_gain_total, se_03_07_losstotal = move_totals(df_03_07_moves)\n",
    "se_08_12_gain_total, se_08_12_losstotal = move_totals(df_08_12_moves)\n",
    "se_13_17_gain_total, se_13_17_losstotal = move_totals(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "se_03_07_gain_total.to_csv(\"data/direct/gaintotal_p0.csv\")\n",
    "se_03_07_losstotal.to_csv(\"data/direct/losstotal_p0.csv\")\n",
    "\n",
    "se_08_12_gain_total.to_csv(\"data/direct/gaintotal_p1.csv\")\n",
    "se_08_12_losstotal.to_csv(\"data/direct/losstotal_p1.csv\")\n",
    "\n",
    "se_13_17_gain_total.to_csv(\"data/direct/gaintotal_p2.csv\")\n",
    "se_13_17_losstotal.to_csv(\"data/direct/losstotal_p2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Almost no origfips to same destfips moves now, likely because duplicate effdates were removed\\\n",
    "Note2: Failed geocoded fips kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute matrices\n",
    "\n",
    "def matrix(df_moves):\n",
    "    se_indices = (\n",
    "        pd.concat([df_moves['origfips'], df_moves['destfips']])\n",
    "            .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    df_matrix = (\n",
    "        df_moves.groupby(['origfips', 'destfips'])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reindex(se_indices, columns=se_indices, fill_value=0)\n",
    "    )\n",
    "\n",
    "    return df_matrix\n",
    "\n",
    "df_03_07_matrix = matrix(df_03_07_moves)\n",
    "df_08_12_matrix = matrix(df_08_12_moves)\n",
    "df_13_17_matrix = matrix(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_matrix.to_csv(\"data/direct/matrix_p0.csv\")\n",
    "df_08_12_matrix.to_csv(\"data/direct/matrix_p1.csv\")\n",
    "df_13_17_matrix.to_csv(\"data/direct/matrix_p2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute high gains & losses\n",
    "\n",
    "def high_moves(df_matrix):\n",
    "    df_zeroed_matrix = df_matrix.copy(deep=True)\n",
    "\n",
    "    np.fill_diagonal(df_zeroed_matrix.values, 0)\n",
    "\n",
    "    # Sum across rows and columns\n",
    "    df_high_loss = df_zeroed_matrix.sum(axis=1).to_frame(name='count')\n",
    "    df_high_gain = df_zeroed_matrix.sum().to_frame(name='count')\n",
    "\n",
    "    # Label fips based on gainers & losers\n",
    "    condList_high_loss = [\n",
    "        df_high_loss.index.isin(se_gainers), df_high_loss.index.isin(se_losers)\n",
    "    ]\n",
    "    condList_high_gain = [\n",
    "        df_high_gain.index.isin(se_gainers), df_high_gain.index.isin(se_losers)\n",
    "    ]\n",
    "    choiceList_high = ['gain', 'loss']\n",
    "\n",
    "    df_high_loss['type'] = pd.Categorical(\n",
    "        np.select(condList_high_loss, choiceList_high, default='other')\n",
    "    )\n",
    "    df_high_gain['type'] = pd.Categorical(\n",
    "        np.select(condList_high_gain, choiceList_high, default='other')\n",
    "    )\n",
    "\n",
    "    return df_high_loss, df_high_gain\n",
    "\n",
    "df_03_07_high_loss, df_03_07_high_gain = high_moves(df_03_07_matrix)\n",
    "df_08_12_high_loss, df_08_12_high_gain = high_moves(df_08_12_matrix)\n",
    "df_13_17_high_loss, df_13_17_high_gain = high_moves(df_13_17_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss.to_csv(\"data/direct/out_of_high_loss_p0.csv\")\n",
    "df_03_07_high_gain.to_csv(\"data/direct/into_high_gain_p0.csv\")\n",
    "\n",
    "df_08_12_high_loss.to_csv(\"data/direct/out_of_high_loss_p1.csv\")\n",
    "df_08_12_high_gain.to_csv(\"data/direct/into_high_gain_p1.csv\")\n",
    "\n",
    "df_13_17_high_loss.to_csv(\"data/direct/out_of_high_loss_p2.csv\")\n",
    "df_13_17_high_gain.to_csv(\"data/direct/into_high_gain_p2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summaries\n",
    "\n",
    "def summary(df_high_moves):\n",
    "    df_summary = pd.DataFrame(columns=['count'])\n",
    "\n",
    "    # Sum each of gainers, losers, and other\n",
    "    df_summary.loc[\"from_loss\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_losers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_gain\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_gainers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_other\"] = (\n",
    "        df_high_moves['count'].sum()\n",
    "        - df_summary.loc[\"from_loss\"]\n",
    "        - df_summary.loc[\"from_gain\"]\n",
    "    )\n",
    "\n",
    "    return df_summary\n",
    "\n",
    "df_03_07_high_loss_summary = summary(df_03_07_high_loss)\n",
    "df_03_07_high_gain_summary = summary(df_03_07_high_gain)\n",
    "\n",
    "df_08_12_high_loss_summary = summary(df_08_12_high_loss)\n",
    "df_08_12_high_gain_summary = summary(df_08_12_high_gain)\n",
    "\n",
    "df_13_17_high_loss_summary = summary(df_13_17_high_loss)\n",
    "df_13_17_high_gain_summary = summary(df_13_17_high_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss_summary.to_csv(\"data/direct/out_of_high_loss_summary_p0.csv\")\n",
    "df_03_07_high_gain_summary.to_csv(\"data/direct/into_high_gain_summary_p0.csv\")\n",
    "\n",
    "df_08_12_high_loss_summary.to_csv(\"data/direct/out_of_high_loss_summary_p1.csv\")\n",
    "df_08_12_high_gain_summary.to_csv(\"data/direct/into_high_gain_summary_p1.csv\")\n",
    "\n",
    "df_13_17_high_loss_summary.to_csv(\"data/direct/out_of_high_loss_summary_p2.csv\")\n",
    "df_13_17_high_gain_summary.to_csv(\"data/direct/into_high_gain_summary_p2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infutor-research-jupyterlab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b170992dff1a2d7a7d34a8e195c66b869f5a2c2d367c7625363ea5e606f09977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
