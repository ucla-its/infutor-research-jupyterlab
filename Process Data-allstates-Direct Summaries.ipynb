{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def dont_dropna(df_areas):\n",
    "    df_moves = df_areas[['effdate', 'fips']]\n",
    "    df_moves = df_moves.rename(columns={'fips': 'destfips'})\n",
    "    df_moves['origfips'] = (\n",
    "        df_moves.groupby(df_moves.index)['destfips']\n",
    "        .shift(fill_value=\"first record\")\n",
    "    )\n",
    "    return df_moves\n",
    "\n",
    "def preshift_dropna(df_areas):\n",
    "    # dropna fips before shifting\n",
    "    df_moves = df_areas[['effdate', 'fips']].dropna()\n",
    "    df_moves = df_moves.rename(columns={'fips': 'destfips'})\n",
    "    df_moves['origfips'] = (\n",
    "        df_moves.groupby(df_moves.index)['destfips']\n",
    "        .shift(fill_value=\"first record\")\n",
    "    )\n",
    "    return df_moves\n",
    "\n",
    "def postshift_dropna(df_areas):\n",
    "    # dropna origfips & destfips\n",
    "    df_moves = df_areas[['effdate', 'fips']]\n",
    "    df_moves = df_moves.rename(columns={'fips': 'destfips'})\n",
    "    df_moves['origfips'] = (\n",
    "        df_moves.groupby(df_moves.index)['destfips']\n",
    "        .shift(fill_value=\"first record\")\n",
    "    )\n",
    "    df_moves = df_moves.dropna()\n",
    "    return df_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Options ###\n",
    "lines_per_chunk = 1_000_000\n",
    "z4types = ('H', 'S', 'R')\n",
    "create_moves = dont_dropna\n",
    "county_codes = (\"037\", \"059\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract all moves ###\n",
    "\n",
    "# Settings\n",
    "usecols_all_states = ['z4type', 'effdate']\n",
    "for i in range(2, 11):\n",
    "    usecols_all_states.append(f'z4type{i}')\n",
    "    usecols_all_states.append(f'effdate{i}')\n",
    "for i in range(1, 11):\n",
    "    usecols_all_states.append(f'fips{i}')\n",
    "\n",
    "dtype_all_states = {\n",
    "    'z4type': 'category',\n",
    "    'effdate': 'float',\n",
    "    'fips1': 'object',\n",
    "}\n",
    "for i in range(2, 11):\n",
    "    dtype_all_states[f'z4type{i}'] = 'category'\n",
    "    dtype_all_states[f'effdate{i}'] = 'float'\n",
    "    dtype_all_states[f'fips{i}'] = 'object'\n",
    "\n",
    "list_dict_col_names = [\n",
    "    {\n",
    "        'z4type': 'z4type',\n",
    "        'effdate': 'effdate',\n",
    "        'fips1': 'fips',\n",
    "    }\n",
    "]\n",
    "for i in range(2, 11):\n",
    "    list_dict_col_names.append(\n",
    "        {\n",
    "            f'z4type{i}': 'z4type',\n",
    "            f'effdate{i}': 'effdate',\n",
    "            f'fips{i}': 'fips',\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Load all_states\n",
    "it_all_states = pd.read_csv(\n",
    "    \"data/all_states.csv\", \n",
    "    usecols=usecols_all_states,\n",
    "    dtype=dtype_all_states,\n",
    "    chunksize=lines_per_chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(index, chunk, finished_chunks, status_file):\n",
    "    if index in finished_chunks:\n",
    "        return f\"Chunk {index} done\" \n",
    "\n",
    "    # Split rows into individual areas\n",
    "    list_df_all_areas = [\n",
    "        chunk[list_dict_col_names[i].keys()].rename(\n",
    "            columns=list_dict_col_names[i]\n",
    "        ) for i in range(10)\n",
    "    ]\n",
    "\n",
    "    # Recombine all areas + sort by effdate\n",
    "    df_all_areas = (\n",
    "        pd.concat(list_df_all_areas)\n",
    "            .dropna(subset=['effdate'])\n",
    "            .sort_values('effdate', kind='stable')\n",
    "    )\n",
    "\n",
    "    # All effdates that do not have a z4type in z4types\n",
    "    bi_all_dropped = ~(\n",
    "        df_all_areas['z4type'].isin(z4types)\n",
    "            .groupby([df_all_areas.index, df_all_areas['effdate']])\n",
    "            .transform('any')\n",
    "    )\n",
    "\n",
    "    # Change values so selected effdates are not removed\n",
    "    df_all_areas.loc[bi_all_dropped, 'z4type'] = 'empty'\n",
    "    df_all_areas.loc[bi_all_dropped, 'fips'] = \"\"\n",
    "\n",
    "    # Filter by Zip+4 type\n",
    "    z4types_mask = (*z4types, 'empty')\n",
    "\n",
    "    df_filtered_areas = df_all_areas[df_all_areas['z4type'].isin(z4types_mask)]\n",
    "\n",
    "    # Choose leftmost fips of each effdate\n",
    "    df_filtered_areas = (\n",
    "        df_filtered_areas.groupby([df_filtered_areas.index, 'effdate']).first()\n",
    "    )\n",
    "    df_filtered_areas = df_filtered_areas.reset_index('effdate')\n",
    "\n",
    "    # Link previous & next areas as moves\n",
    "    df_all_moves = create_moves(df_filtered_areas)\n",
    "\n",
    "    # Filter by county code\n",
    "    df_filtered_moves = df_all_moves[\n",
    "        df_all_moves['origfips'].str[2:5].isin(county_codes) |\n",
    "            df_all_moves['destfips'].str[2:5].isin(county_codes)\n",
    "    ]\n",
    "    \n",
    "    # Separate moves into periods\n",
    "    extract_period = lambda start, stop: df_filtered_moves[\n",
    "        df_filtered_moves['effdate'].between(start, stop, inclusive='left')\n",
    "    ][['origfips', 'destfips']]\n",
    "\n",
    "    df_03_07_moves = extract_period(200301, 200801)\n",
    "    df_08_12_moves = extract_period(200801, 201301)\n",
    "    df_13_17_moves = extract_period(201301, 201801)\n",
    "\n",
    "    # Write to files\n",
    "    df_03_07_moves.to_csv(\n",
    "        f\"data/direct/03_07_moves-{lines_per_chunk}/{index}.csv\", index=False\n",
    "    )\n",
    "    df_08_12_moves.to_csv(\n",
    "        f\"data/direct/08_12_moves-{lines_per_chunk}/{index}.csv\", index=False\n",
    "    )\n",
    "    df_13_17_moves.to_csv(\n",
    "        f\"data/direct/13_17_moves-{lines_per_chunk}/{index}.csv\", index=False\n",
    "    )\n",
    "\n",
    "    # Update status\n",
    "    status_file.write(f\"{index}\\n\")\n",
    "\n",
    "    return f\"Chunk {index} done\"\n",
    "\n",
    "\n",
    "# Create status file\n",
    "try:\n",
    "    f = open(f\"direct_status-{lines_per_chunk}.txt\")\n",
    "except FileNotFoundError:\n",
    "    f = open(f\"direct_status-{lines_per_chunk}.txt\", 'a')\n",
    "else:\n",
    "    finished_chunks = {int(line) for line in f.readlines()}\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# Create chunk folders\n",
    "Path(f\"data/direct/03_07_moves-{lines_per_chunk}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "Path(f\"data/direct/08_12_moves-{lines_per_chunk}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "Path(f\"data/direct/13_17_moves-{lines_per_chunk}\").mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "\n",
    "# Process all states\n",
    "with open(f\"direct_status-{lines_per_chunk}.txt\", 'a') as status_file:\n",
    "    for index, chunk in enumerate(it_all_states):\n",
    "        process_chunk(index, chunk, finished_chunks, status_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load all moves ###\n",
    "\n",
    "# Settings\n",
    "dtype_all_moves = {\n",
    "    'origfips': 'object',\n",
    "    'destfips': 'object',\n",
    "}\n",
    "\n",
    "\n",
    "# Read chunks\n",
    "df_03_07_moves = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(chunk_file, dtype=dtype_all_moves)\n",
    "        for chunk_file in glob(\n",
    "            f\"data/direct/03_07_moves-{lines_per_chunk}/*.csv\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "df_08_12_moves = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(chunk_file, dtype=dtype_all_moves)\n",
    "        for chunk_file in glob(\n",
    "            f\"data/direct/08_12_moves-{lines_per_chunk}/*.csv\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "df_13_17_moves = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(chunk_file, dtype=dtype_all_moves)\n",
    "        for chunk_file in glob(\n",
    "            f\"data/direct/13_17_moves-{lines_per_chunk}/*.csv\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data analysis ###\n",
    "\n",
    "# Settings\n",
    "usecols_fips_tract = ['tractid_fips', 'gainers', 'losers']\n",
    "\n",
    "dtype_fips_tract = {\n",
    "    'tractid_fips': 'object',\n",
    "    'gainers': 'bool',\n",
    "    'losers': 'bool',\n",
    "}\n",
    "\n",
    "\n",
    "# Load fips_tracts_cats\n",
    "df_fips_tract = pd.read_csv(\n",
    "    \"fips_tracts_cats.csv\",\n",
    "    usecols=usecols_fips_tract,\n",
    "    dtype=dtype_fips_tract\n",
    ")\n",
    "\n",
    "# Get Series of gainers and losers\n",
    "se_gainers = df_fips_tract[df_fips_tract['gainers']]['tractid_fips']\n",
    "se_losers = df_fips_tract[df_fips_tract['losers']]['tractid_fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gain & loss totals\n",
    "\n",
    "def move_totals(df_moves):\n",
    "    se_gain_total = (\n",
    "        df_moves.groupby('destfips')\n",
    "            .size()\n",
    "            .reindex(se_gainers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    se_loss_total = (\n",
    "        df_moves.groupby('origfips')\n",
    "            .size()\n",
    "            .reindex(se_losers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    return se_gain_total, se_loss_total\n",
    "\n",
    "se_03_07_gain_total, se_03_07_loss_total = move_totals(df_03_07_moves)\n",
    "se_08_12_gain_total, se_08_12_loss_total = move_totals(df_08_12_moves)\n",
    "se_13_17_gain_total, se_13_17_loss_total = move_totals(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "se_03_07_gain_total.to_csv(\"data/direct/03_07_gain_total.csv\")\n",
    "se_03_07_loss_total.to_csv(\"data/direct/03_07_loss_total.csv\")\n",
    "\n",
    "se_08_12_gain_total.to_csv(\"data/direct/03_07_gain_total.csv\")\n",
    "se_08_12_loss_total.to_csv(\"data/direct/03_07_loss_total.csv\")\n",
    "\n",
    "se_13_17_gain_total.to_csv(\"data/direct/03_07_gain_total.csv\")\n",
    "se_13_17_loss_total.to_csv(\"data/direct/03_07_loss_total.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Almost no origfips to same destfips moves now, likely because duplicate effdates were removed\\\n",
    "Note2: Failed geocoded fips kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute matrices\n",
    "\n",
    "def matrix(df_moves):\n",
    "    se_indices = (\n",
    "        pd.concat([df_moves['origfips'], df_moves['destfips']])\n",
    "            .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    df_matrix = (\n",
    "        df_moves.groupby(['origfips', 'destfips'])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reindex(se_indices, columns=se_indices, fill_value=0)\n",
    "    )\n",
    "\n",
    "    return df_matrix\n",
    "\n",
    "df_03_07_matrix = matrix(df_03_07_moves)\n",
    "df_08_12_matrix = matrix(df_08_12_moves)\n",
    "df_13_17_matrix = matrix(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_matrix.to_csv(\"data/direct/03_07_matrix.csv\")\n",
    "df_08_12_matrix.to_csv(\"data/direct/08_12_matrix.csv\")\n",
    "df_13_17_matrix.to_csv(\"data/direct/13_17_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute high gains & losses\n",
    "\n",
    "def high_moves(df_matrix):\n",
    "    df_zeroed_matrix = df_matrix.copy(deep=True)\n",
    "\n",
    "    np.fill_diagonal(df_zeroed_matrix.values, 0)\n",
    "\n",
    "    # Sum across rows and columns\n",
    "    df_high_loss = df_zeroed_matrix.sum(axis=1).to_frame(name='count')\n",
    "    df_high_gain = df_zeroed_matrix.sum().to_frame(name='count')\n",
    "\n",
    "    # Label fips based on gainers & losers\n",
    "    condList_high_loss = [\n",
    "        df_high_loss.index.isin(se_gainers), df_high_loss.index.isin(se_losers)\n",
    "    ]\n",
    "    condList_high_gain = [\n",
    "        df_high_gain.index.isin(se_gainers), df_high_gain.index.isin(se_losers)\n",
    "    ]\n",
    "    choiceList_high = ['gain', 'loss']\n",
    "\n",
    "    df_high_loss['type'] = pd.Categorical(\n",
    "        np.select(condList_high_loss, choiceList_high, default='other')\n",
    "    )\n",
    "    df_high_gain['type'] = pd.Categorical(\n",
    "        np.select(condList_high_gain, choiceList_high, default='other')\n",
    "    )\n",
    "\n",
    "    return df_high_loss, df_high_gain\n",
    "\n",
    "df_03_07_high_loss, df_03_07_high_gain = high_moves(df_03_07_matrix)\n",
    "df_08_12_high_loss, df_08_12_high_gain = high_moves(df_08_12_matrix)\n",
    "df_13_17_high_loss, df_13_17_high_gain = high_moves(df_13_17_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss.to_csv(\"data/direct/03_07_out_of_high_loss.csv\")\n",
    "df_03_07_high_gain.to_csv(\"data/direct/03_07_into_high_gain.csv\")\n",
    "\n",
    "df_08_12_high_loss.to_csv(\"data/direct/08_12_out_of_high_loss.csv\")\n",
    "df_08_12_high_gain.to_csv(\"data/direct/08_12_into_high_gain.csv\")\n",
    "\n",
    "df_13_17_high_loss.to_csv(\"data/direct/13_17_out_of_high_loss.csv\")\n",
    "df_13_17_high_gain.to_csv(\"data/direct/13_17_into_high_gain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summaries\n",
    "\n",
    "def summary(df_high_moves):\n",
    "    df_summary = pd.DataFrame(columns=['count'])\n",
    "    \n",
    "    # Sum each of gainers, losers, and other\n",
    "    df_summary.loc[\"from_loss\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_losers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_gain\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_gainers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_other\"] = (\n",
    "        df_high_moves['count'].sum()\n",
    "        - df_summary.loc[\"from_loss\"]\n",
    "        - df_summary.loc[\"from_gain\"]\n",
    "    )\n",
    "\n",
    "    return df_summary\n",
    "\n",
    "df_03_07_high_loss_summary = summary(df_03_07_high_loss)\n",
    "df_03_07_high_gain_summary = summary(df_03_07_high_gain)\n",
    "\n",
    "df_08_12_high_loss_summary = summary(df_08_12_high_loss)\n",
    "df_08_12_high_gain_summary = summary(df_08_12_high_gain)\n",
    "\n",
    "df_13_17_high_loss_summary = summary(df_13_17_high_loss)\n",
    "df_13_17_high_gain_summary = summary(df_13_17_high_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss_summary.to_csv(\"data/direct/03_07_out_of_high_loss_summary.csv\")\n",
    "df_03_07_high_gain_summary.to_csv(\"data/direct/03_07_into_high_gain_summary.csv\")\n",
    "\n",
    "df_08_12_high_loss_summary.to_csv(\"data/direct/08_12_out_of_high_loss_summary.csv\")\n",
    "df_08_12_high_gain_summary.to_csv(\"data/direct/08_12_into_high_gain_summary.csv\")\n",
    "\n",
    "df_13_17_high_loss_summary.to_csv(\"data/direct/13_17_out_of_high_loss_summary.csv\")\n",
    "df_13_17_high_gain_summary.to_csv(\"data/direct/13_17_into_high_gain_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infutor-research-jupyterlab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b170992dff1a2d7a7d34a8e195c66b869f5a2c2d367c7625363ea5e606f09977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
