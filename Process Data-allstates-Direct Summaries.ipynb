{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Options ###\n",
    "z4types = ('H', 'S', 'R')\n",
    "county_codes = (\"037\", \"059\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract all moves ###\n",
    "\n",
    "# Settings\n",
    "usecols_all_states = ['z4type', 'effdate']\n",
    "for i in range(2, 11):\n",
    "    usecols_all_states.append(f'z4type{i}')\n",
    "    usecols_all_states.append(f'effdate{i}')\n",
    "for i in range(1, 11):\n",
    "    usecols_all_states.append(f'fips{i}')\n",
    "\n",
    "dtype_all_states = {\n",
    "    'z4type': 'object',\n",
    "    'effdate': 'float',\n",
    "    'fips1': 'object',\n",
    "}\n",
    "for i in range(2, 11):\n",
    "    dtype_all_states[f'z4type{i}'] = 'object'\n",
    "    dtype_all_states[f'effdate{i}'] = 'float'\n",
    "    dtype_all_states[f'fips{i}'] = 'object'\n",
    "\n",
    "\n",
    "# DASK: Load all_states\n",
    "dd_all_states = dd.read_csv(\n",
    "    \"data/all_states.csv\", \n",
    "    usecols=usecols_all_states,\n",
    "    dtype=dtype_all_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "df_all_states = pd.read_csv(\n",
    "    \"data/all_states.csv\", \n",
    "    usecols=usecols_all_states,\n",
    "    dtype=dtype_all_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  z4type  effdate fips\n",
      "1    foo      1.0  foo\n",
      "  z4type  effdate fips\n",
      "2    foo      1.0  foo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tedzy\\AppData\\Local\\Temp\\ipykernel_3952\\993976052.py:35: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_all_areas_dropna.groupby([dd_all_areas.index, dd_all_areas['effdate']]).apply(reduce_effdate)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "groupby-apply with a multiple Series is currently not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\Process Data-allstates-Direct Summaries.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Direct%20Summaries.ipynb#ch0000004?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreduce_effdate\u001b[39m(group):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Direct%20Summaries.ipynb#ch0000004?line=32'>33</a>\u001b[0m     \u001b[39mprint\u001b[39m(group)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Direct%20Summaries.ipynb#ch0000004?line=34'>35</a>\u001b[0m dd_all_areas_dropna\u001b[39m.\u001b[39;49mgroupby([dd_all_areas\u001b[39m.\u001b[39;49mindex, dd_all_areas[\u001b[39m'\u001b[39;49m\u001b[39meffdate\u001b[39;49m\u001b[39m'\u001b[39;49m]])\u001b[39m.\u001b[39;49mapply(reduce_effdate)\n",
      "File \u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\lib\\site-packages\\dask\\dataframe\\groupby.py:1753\u001b[0m, in \u001b[0;36m_GroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[39m# Validate self.by\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mby, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[0;32m   1751\u001b[0m     \u001b[39misinstance\u001b[39m(item, Series) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mby\n\u001b[0;32m   1752\u001b[0m ):\n\u001b[1;32m-> 1753\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1754\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgroupby-apply with a multiple Series is currently not supported\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1755\u001b[0m     )\n\u001b[0;32m   1757\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\n\u001b[0;32m   1758\u001b[0m should_shuffle \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m (df\u001b[39m.\u001b[39mknown_divisions \u001b[39mand\u001b[39;00m df\u001b[39m.\u001b[39m_contains_index_name(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mby))\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: groupby-apply with a multiple Series is currently not supported"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "list_dict_col_names = [\n",
    "    {\n",
    "        'z4type': 'z4type',\n",
    "        'effdate': 'effdate',\n",
    "        'fips1': 'fips',\n",
    "    }\n",
    "]\n",
    "for i in range(2, 11):\n",
    "    list_dict_col_names.append(\n",
    "        {\n",
    "            f'z4type{i}': 'z4type',\n",
    "            f'effdate{i}': 'effdate',\n",
    "            f'fips{i}': 'fips',\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# DASK: Split rows into individual areas\n",
    "list_dd_all_areas = [\n",
    "    dd_all_states[[*list_dict_col_names[i].keys()]].rename(\n",
    "        columns=list_dict_col_names[i]\n",
    "    ) for i in range(10)\n",
    "]\n",
    "\n",
    "# Recombine all areas\n",
    "dd_all_areas = dd.concat(list_dd_all_areas)\n",
    "\n",
    "\n",
    "def reduce_effdate(group):\n",
    "    print(group)\n",
    "\n",
    "dd_all_areas.groupby([dd_all_areas.index, dd_all_areas['effdate']], dropna=True).apply(reduce_effdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS: Split rows into individual areas\n",
    "list_df_all_areas = [\n",
    "    df_all_states[list_dict_col_names[i].keys()].rename(\n",
    "        columns=list_dict_col_names[i]\n",
    "    ) for i in range(10)\n",
    "]\n",
    "\n",
    "# Recombine all areas + sort effdate\n",
    "df_all_areas = pd.concat(list_df_all_areas)\n",
    "\n",
    "df_all_areas = (\n",
    "    df_all_areas.dropna(subset='effdate')\n",
    "        .sort_values('effdate', kind='stable')\n",
    ")\n",
    "\n",
    "# All effdates that do not have a z4type in z4types\n",
    "bi_all_dropped = ~(\n",
    "    df_all_areas['z4type'].isin(z4types)\n",
    "        .groupby([df_all_areas.index, df_all_areas['effdate']])\n",
    "        .transform('any')\n",
    ")\n",
    "\n",
    "# Change values so selected effdates are not removed\n",
    "df_all_areas.loc[bi_all_dropped, 'z4type'] = 'empty'\n",
    "df_all_areas.loc[bi_all_dropped, 'fips'] = \"\"\n",
    "\n",
    "# Filter by Zip+4 type\n",
    "z4types_mask = (*z4types, 'empty')\n",
    "\n",
    "df_filtered_areas = df_all_areas[\n",
    "    df_all_areas['z4type'].isin(z4types_mask)\n",
    "]\n",
    "\n",
    "# Choose leftmost fips of each effdate\n",
    "df_filtered_areas = (\n",
    "    df_filtered_areas.groupby([df_filtered_areas.index, 'effdate']).first()\n",
    ")\n",
    "df_filtered_areas = df_filtered_areas.reset_index('effdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the below cells &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link previous & next areas as moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna fips before shifting\n",
    "df_all_moves = df_filtered_areas[['effdate', 'fips']].dropna()\n",
    "df_all_moves = df_all_moves.rename(columns={'fips': 'destfips'})\n",
    "df_all_moves['origfips'] = (\n",
    "    df_all_moves.groupby(df_all_moves.index)['destfips']\n",
    "    .shift(fill_value=\"first record\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna origfips & destfips\n",
    "df_all_moves = df_filtered_areas[['effdate', 'fips']]\n",
    "df_all_moves = df_all_moves.rename(columns={'fips': 'destfips'})\n",
    "df_all_moves['origfips'] = (\n",
    "    df_all_moves.groupby(df_all_moves.index)['destfips']\n",
    "    .shift(fill_value=\"first record\")\n",
    ")\n",
    "df_all_moves = df_all_moves.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't dropna\n",
    "df_all_moves = df_filtered_areas[['effdate', 'fips']]\n",
    "df_all_moves = df_all_moves.rename(columns={'fips': 'destfips'})\n",
    "df_all_moves['origfips'] = (\n",
    "    df_all_moves.groupby(df_all_moves.index)['destfips']\n",
    "    .shift(fill_value=\"first record\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of the above cells &uarr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by county code\n",
    "df_filtered_moves = df_all_moves[\n",
    "    df_all_moves['origfips'].str[2:5].isin(county_codes) |\n",
    "        df_all_moves['destfips'].str[2:5].isin(county_codes)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate moves into periods\n",
    "\n",
    "def extract_period(start, stop):\n",
    "    return df_filtered_moves[\n",
    "        df_filtered_moves['effdate'].between(start, stop, inclusive='left')\n",
    "    ][['origfips', 'destfips']]\n",
    "\n",
    "df_03_07_moves = extract_period(200301, 200801)\n",
    "df_08_12_moves = extract_period(200801, 201301)\n",
    "df_13_17_moves = extract_period(201301, 201801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_to_csv = {\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "df_03_07_moves.to_csv(\"data/direct/03_07_moves.csv\", **arg_to_csv)\n",
    "df_08_12_moves.to_csv(\"data/direct/08_12_moves.csv\", **arg_to_csv)\n",
    "df_13_17_moves.to_csv(\"data/direct/13_17_moves.csv\", **arg_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data analysis ###\n",
    "\n",
    "# Settings\n",
    "usecols_fips_tract = ['tractid_fips', 'gainers', 'losers']\n",
    "\n",
    "dtype_fips_tract = {\n",
    "    'tractid_fips': 'object',\n",
    "    'gainers': 'bool',\n",
    "    'losers': 'bool',\n",
    "}\n",
    "\n",
    "\n",
    "# Load fips_tracts_cats\n",
    "df_fips_tract = pd.read_csv(\n",
    "    \"fips_tracts_cats.csv\",\n",
    "    usecols=usecols_fips_tract,\n",
    "    dtype=dtype_fips_tract\n",
    ")\n",
    "\n",
    "# Get Series of gainers and losers\n",
    "se_gainers = df_fips_tract[df_fips_tract['gainers']]['tractid_fips']\n",
    "se_losers = df_fips_tract[df_fips_tract['losers']]['tractid_fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gain & loss totals\n",
    "\n",
    "def move_totals(df_moves):\n",
    "    se_gain_total = (\n",
    "        df_moves.groupby('destfips')\n",
    "            .size()\n",
    "            .reindex(se_gainers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    se_loss_total = (\n",
    "        df_moves.groupby('origfips')\n",
    "            .size()\n",
    "            .reindex(se_losers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    return se_gain_total, se_loss_total\n",
    "\n",
    "se_03_07_gain_total, se_03_07_loss_total = move_totals(df_03_07_moves)\n",
    "se_08_12_gain_total, se_08_12_loss_total = move_totals(df_08_12_moves)\n",
    "se_13_17_gain_total, se_13_17_loss_total = move_totals(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "se_03_07_gain_total.to_csv(\"data/direct/gaintotal_p0.csv\")\n",
    "se_03_07_loss_total.to_csv(\"data/direct/losstotal_p0.csv\")\n",
    "\n",
    "se_08_12_gain_total.to_csv(\"data/direct/gaintotal_p1.csv\")\n",
    "se_08_12_loss_total.to_csv(\"data/direct/losstotal_p1.csv\")\n",
    "\n",
    "se_13_17_gain_total.to_csv(\"data/direct/gaintotal_p2.csv\")\n",
    "se_13_17_loss_total.to_csv(\"data/direct/losstotal_p2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Almost no origfips to same destfips moves now, likely because duplicate effdates were removed\\\n",
    "Note2: Failed geocoded fips kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute matrices\n",
    "\n",
    "def matrix(df_moves):\n",
    "    se_indices = (\n",
    "        pd.concat([df_moves['origfips'], df_moves['destfips']])\n",
    "            .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    df_matrix = (\n",
    "        df_moves.groupby(['origfips', 'destfips'])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reindex(se_indices, columns=se_indices, fill_value=0)\n",
    "    )\n",
    "\n",
    "    return df_matrix\n",
    "\n",
    "df_03_07_matrix = matrix(df_03_07_moves)\n",
    "df_08_12_matrix = matrix(df_08_12_moves)\n",
    "df_13_17_matrix = matrix(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_matrix.to_csv(\"data/direct/matrix_p0.csv\")\n",
    "df_08_12_matrix.to_csv(\"data/direct/matrix_p1.csv\")\n",
    "df_13_17_matrix.to_csv(\"data/direct/matrix_p2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute high gains & losses\n",
    "\n",
    "def high_moves(df_matrix):\n",
    "    df_zeroed_matrix = df_matrix.copy(deep=True)\n",
    "\n",
    "    np.fill_diagonal(df_zeroed_matrix.values, 0)\n",
    "\n",
    "    # Sum across rows and columns\n",
    "    df_high_loss = df_zeroed_matrix.sum(axis=1).to_frame(name='count')\n",
    "    df_high_gain = df_zeroed_matrix.sum().to_frame(name='count')\n",
    "\n",
    "    # Label fips based on gainers & losers\n",
    "    condList_high_loss = [\n",
    "        df_high_loss.index.isin(se_gainers), df_high_loss.index.isin(se_losers)\n",
    "    ]\n",
    "    condList_high_gain = [\n",
    "        df_high_gain.index.isin(se_gainers), df_high_gain.index.isin(se_losers)\n",
    "    ]\n",
    "    choiceList_high = ['gain', 'loss']\n",
    "\n",
    "    df_high_loss['type'] = pd.Categorical(\n",
    "        np.select(condList_high_loss, choiceList_high, default='other')\n",
    "    )\n",
    "    df_high_gain['type'] = pd.Categorical(\n",
    "        np.select(condList_high_gain, choiceList_high, default='other')\n",
    "    )\n",
    "\n",
    "    return df_high_loss, df_high_gain\n",
    "\n",
    "df_03_07_high_loss, df_03_07_high_gain = high_moves(df_03_07_matrix)\n",
    "df_08_12_high_loss, df_08_12_high_gain = high_moves(df_08_12_matrix)\n",
    "df_13_17_high_loss, df_13_17_high_gain = high_moves(df_13_17_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss.to_csv(\"data/direct/out_of_high_loss_p0.csv\")\n",
    "df_03_07_high_gain.to_csv(\"data/direct/into_high_gain_p0.csv\")\n",
    "\n",
    "df_08_12_high_loss.to_csv(\"data/direct/out_of_high_loss_p1.csv\")\n",
    "df_08_12_high_gain.to_csv(\"data/direct/into_high_gain_p1.csv\")\n",
    "\n",
    "df_13_17_high_loss.to_csv(\"data/direct/out_of_high_loss_p2.csv\")\n",
    "df_13_17_high_gain.to_csv(\"data/direct/into_high_gain_p2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summaries\n",
    "\n",
    "def summary(df_high_moves):\n",
    "    df_summary = pd.DataFrame(columns=['count'])\n",
    "    \n",
    "    # Sum each of gainers, losers, and other\n",
    "    df_summary.loc[\"from_loss\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_losers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_gain\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_gainers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_other\"] = (\n",
    "        df_high_moves['count'].sum()\n",
    "        - df_summary.loc[\"from_loss\"]\n",
    "        - df_summary.loc[\"from_gain\"]\n",
    "    )\n",
    "\n",
    "    return df_summary\n",
    "\n",
    "df_03_07_high_loss_summary = summary(df_03_07_high_loss)\n",
    "df_03_07_high_gain_summary = summary(df_03_07_high_gain)\n",
    "\n",
    "df_08_12_high_loss_summary = summary(df_08_12_high_loss)\n",
    "df_08_12_high_gain_summary = summary(df_08_12_high_gain)\n",
    "\n",
    "df_13_17_high_loss_summary = summary(df_13_17_high_loss)\n",
    "df_13_17_high_gain_summary = summary(df_13_17_high_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss_summary.to_csv(\"data/direct/out_of_high_loss_summary_p0.csv\")\n",
    "df_03_07_high_gain_summary.to_csv(\"data/direct/into_high_gain_summary_p0.csv\")\n",
    "\n",
    "df_08_12_high_loss_summary.to_csv(\"data/direct/out_of_high_loss_summary_p1.csv\")\n",
    "df_08_12_high_gain_summary.to_csv(\"data/direct/into_high_gain_summary_p1.csv\")\n",
    "\n",
    "df_13_17_high_loss_summary.to_csv(\"data/direct/out_of_high_loss_summary_p2.csv\")\n",
    "df_13_17_high_gain_summary.to_csv(\"data/direct/into_high_gain_summary_p2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infutor-research-jupyterlab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b170992dff1a2d7a7d34a8e195c66b869f5a2c2d367c7625363ea5e606f09977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
