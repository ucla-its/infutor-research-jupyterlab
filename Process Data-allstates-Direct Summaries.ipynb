{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "\n",
    "def dont_dropna(df_areas):\n",
    "    df_moves = df_areas[['effdate', 'fips']]\n",
    "    df_moves = df_moves.rename(columns={'fips': 'destfips'})\n",
    "    df_moves['origfips'] = (\n",
    "        df_moves.groupby(df_moves.index)['destfips']\n",
    "        .shift(fill_value=\"first record\")\n",
    "    )\n",
    "    return df_moves\n",
    "\n",
    "def preshift_dropna(df_areas):\n",
    "    # dropna fips before shifting\n",
    "    df_moves = df_areas[['effdate', 'fips']].dropna()\n",
    "    df_moves = df_moves.rename(columns={'fips': 'destfips'})\n",
    "    df_moves['origfips'] = (\n",
    "        df_moves.groupby(df_moves.index)['destfips']\n",
    "        .shift(fill_value=\"first record\")\n",
    "    )\n",
    "    return df_moves\n",
    "\n",
    "def postshift_dropna(df_areas):\n",
    "    # dropna origfips & destfips\n",
    "    df_moves = df_areas[['effdate', 'fips']]\n",
    "    df_moves = df_moves.rename(columns={'fips': 'destfips'})\n",
    "    df_moves['origfips'] = (\n",
    "        df_moves.groupby(df_moves.index)['destfips']\n",
    "        .shift(fill_value=\"first record\")\n",
    "    )\n",
    "    df_moves = df_moves.dropna()\n",
    "    return df_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Options ###\n",
    "z4types = ('H', 'S', 'R')\n",
    "county_codes = (\"037\", \"059\")\n",
    "create_moves = dont_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract all moves ###\n",
    "\n",
    "# Settings\n",
    "usecols_all_states = ['z4type', 'effdate']\n",
    "for i in range(2, 11):\n",
    "    usecols_all_states.append(f'z4type{i}')\n",
    "    usecols_all_states.append(f'effdate{i}')\n",
    "for i in range(1, 11):\n",
    "    usecols_all_states.append(f'fips{i}')\n",
    "\n",
    "dtype_all_states = {\n",
    "    'z4type': 'category',\n",
    "    'effdate': 'float',\n",
    "    'fips1': 'object',\n",
    "}\n",
    "for i in range(2, 11):\n",
    "    dtype_all_states[f'z4type{i}'] = 'category'\n",
    "    dtype_all_states[f'effdate{i}'] = 'float'\n",
    "    dtype_all_states[f'fips{i}'] = 'object'\n",
    "\n",
    "list_dict_col_names = [\n",
    "    {\n",
    "        'z4type': 'z4type',\n",
    "        'effdate': 'effdate',\n",
    "        'fips1': 'fips',\n",
    "    }\n",
    "]\n",
    "for i in range(2, 11):\n",
    "    list_dict_col_names.append(\n",
    "        {\n",
    "            f'z4type{i}': 'z4type',\n",
    "            f'effdate{i}': 'effdate',\n",
    "            f'fips{i}': 'fips',\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Load all_states\n",
    "it_all_states = pd.read_csv(\n",
    "    \"data/all_states.csv\", \n",
    "    usecols=usecols_all_states,\n",
    "    dtype=dtype_all_states,\n",
    "    chunksize=1000000\n",
    ")\n",
    "\n",
    "def process_chunk(indexed_chunk, finished_chunks, status_file):\n",
    "    index, chunk = indexed_chunk\n",
    "\n",
    "    if index in finished_chunks:\n",
    "        return f\"Chunk {index} done\" \n",
    "\n",
    "    # Split rows into individual areas\n",
    "    list_df_all_areas = [\n",
    "        chunk[list_dict_col_names[i].keys()].rename(\n",
    "            columns=list_dict_col_names[i]\n",
    "        ) for i in range(10)\n",
    "    ]\n",
    "\n",
    "    # Recombine all areas + sort by effdate\n",
    "    df_all_areas = (\n",
    "        pd.concat(list_df_all_areas)\n",
    "            .dropna(subset=['effdate'])\n",
    "            .sort_values('effdate', kind='stable')\n",
    "    )\n",
    "\n",
    "    # All effdates that do not have a z4type in z4types\n",
    "    bi_all_dropped = ~(\n",
    "        df_all_areas['z4type'].isin(z4types)\n",
    "            .groupby([df_all_areas.index, df_all_areas['effdate']])\n",
    "            .transform('any')\n",
    "    )\n",
    "\n",
    "    # Change values so selected effdates are not removed\n",
    "    df_all_areas.loc[bi_all_dropped, 'z4type'] = 'empty'\n",
    "    df_all_areas.loc[bi_all_dropped, 'fips'] = \"\"\n",
    "\n",
    "    # Filter by Zip+4 type\n",
    "    z4types_mask = (*z4types, 'empty')\n",
    "\n",
    "    df_filtered_areas = df_all_areas[df_all_areas['z4type'].isin(z4types_mask)]\n",
    "\n",
    "    # Choose leftmost fips of each effdate\n",
    "    df_filtered_areas = (\n",
    "        df_filtered_areas.groupby([df_filtered_areas.index, 'effdate']).first()\n",
    "    )\n",
    "    df_filtered_areas = df_filtered_areas.reset_index('effdate')\n",
    "\n",
    "    # Link previous & next areas as moves\n",
    "    df_all_moves = create_moves(df_filtered_areas)\n",
    "\n",
    "    # Filter by county code\n",
    "    df_filtered_moves = df_all_moves[\n",
    "        df_all_moves['origfips'].str[2:5].isin(county_codes) |\n",
    "            df_all_moves['destfips'].str[2:5].isin(county_codes)\n",
    "    ]\n",
    "    \n",
    "    # Separate moves into periods\n",
    "    extract_period = lambda start, stop: df_filtered_moves[\n",
    "        df_filtered_moves['effdate'].between(start, stop, inclusive='left')\n",
    "    ][['origfips', 'destfips']]\n",
    "\n",
    "    df_03_07_moves = extract_period(200301, 200801)\n",
    "    df_08_12_moves = extract_period(200801, 201301)\n",
    "    df_13_17_moves = extract_period(201301, 201801)\n",
    "\n",
    "    df_03_07_moves.to_csv(f\"data/direct/03_07_moves/{index}.csv\", index=False)\n",
    "    df_08_12_moves.to_csv(f\"data/direct/08_12_moves/{index}.csv\", index=False)\n",
    "    df_13_17_moves.to_csv(f\"data/direct/13_17_moves/{index}.csv\", index=False)\n",
    "\n",
    "    status_file.write(f\"{index}\\n\")\n",
    "\n",
    "    return f\"Chunk {index} done\"\n",
    "\n",
    "with open(\"direct_status.txt\") as f:\n",
    "    finished_chunks = {int(line) for line in f.readlines()}\n",
    "\n",
    "with open(\"direct_status.txt\", 'a') as status_file:\n",
    "    for indexed_chunk in enumerate(it_all_states):\n",
    "        process_chunk(indexed_chunk, finished_chunks, status_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'data/direct/03_07_moves/.ipynb_checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10606/4074760407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"data/direct/03_07_moves/{chunk_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/direct/03_07_moves\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_10606/4074760407.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"data/direct/03_07_moves/{chunk_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/direct/03_07_moves\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'data/direct/03_07_moves/.ipynb_checkpoints'"
     ]
    }
   ],
   "source": [
    "df_03_07_moves = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(f\"data/direct/03_07_moves/{chunk_file}\")\n",
    "        for chunk_file in listdir(\"data/direct/03_07_moves\")\n",
    "    ]\n",
    ")\n",
    "df_08_12_moves = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(f\"data/direct/08_12_moves/{chunk_file}\")\n",
    "        for chunk_file in listdir(\"data/direct/08_12_moves\")\n",
    "    ]\n",
    ")\n",
    "df_13_17_moves = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(f\"data/direct/13_17_moves/{chunk_file}\")\n",
    "        for chunk_file in listdir(\"data/direct/13_17_moves\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data analysis ###\n",
    "\n",
    "# Settings\n",
    "usecols_fips_tract = ['tractid_fips', 'gainers', 'losers']\n",
    "\n",
    "dtype_fips_tract = {\n",
    "    'tractid_fips': 'object',\n",
    "    'gainers': 'bool',\n",
    "    'losers': 'bool',\n",
    "}\n",
    "\n",
    "\n",
    "# Load fips_tracts_cats\n",
    "df_fips_tract = pd.read_csv(\n",
    "    \"fips_tracts_cats.csv\",\n",
    "    usecols=usecols_fips_tract,\n",
    "    dtype=dtype_fips_tract\n",
    ")\n",
    "\n",
    "# Get Series of gainers and losers\n",
    "se_gainers = df_fips_tract[df_fips_tract['gainers']]['tractid_fips']\n",
    "se_losers = df_fips_tract[df_fips_tract['losers']]['tractid_fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gain & loss totals\n",
    "\n",
    "def move_totals(df_moves):\n",
    "    se_gain_total = (\n",
    "        df_moves.groupby('destfips')\n",
    "            .size()\n",
    "            .reindex(se_gainers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    se_loss_total = (\n",
    "        df_moves.groupby('origfips')\n",
    "            .size()\n",
    "            .reindex(se_losers, fill_value=0)\n",
    "            .rename('count')\n",
    "    )\n",
    "    return se_gain_total, se_loss_total\n",
    "\n",
    "se_03_07_gain_total, se_03_07_loss_total = move_totals(df_03_07_moves)\n",
    "se_08_12_gain_total, se_08_12_loss_total = move_totals(df_08_12_moves)\n",
    "se_13_17_gain_total, se_13_17_loss_total = move_totals(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "se_03_07_gain_total.to_csv(\"data/direct/03_07_gain_total.csv\")\n",
    "se_03_07_loss_total.to_csv(\"data/direct/03_07_loss_total.csv\")\n",
    "\n",
    "se_08_12_gain_total.to_csv(\"data/direct/03_07_gain_total.csv\")\n",
    "se_08_12_loss_total.to_csv(\"data/direct/03_07_loss_total.csv\")\n",
    "\n",
    "se_13_17_gain_total.to_csv(\"data/direct/03_07_gain_total.csv\")\n",
    "se_13_17_loss_total.to_csv(\"data/direct/03_07_loss_total.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Almost no origfips to same destfips moves now, likely because duplicate effdates were removed\\\n",
    "Note2: Failed geocoded fips kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute matrices\n",
    "\n",
    "def matrix(df_moves):\n",
    "    se_indices = (\n",
    "        pd.concat([df_moves['origfips'], df_moves['destfips']])\n",
    "            .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    df_matrix = (\n",
    "        df_moves.groupby(['origfips', 'destfips'])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            .reindex(se_indices, columns=se_indices, fill_value=0)\n",
    "    )\n",
    "\n",
    "    return df_matrix\n",
    "\n",
    "df_03_07_matrix = matrix(df_03_07_moves)\n",
    "df_08_12_matrix = matrix(df_08_12_moves)\n",
    "df_13_17_matrix = matrix(df_13_17_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_matrix.to_csv(\"data/direct/03_07_matrix.csv\")\n",
    "df_08_12_matrix.to_csv(\"data/direct/08_12_matrix.csv\")\n",
    "df_13_17_matrix.to_csv(\"data/direct/13_17_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute high gains & losses\n",
    "\n",
    "def high_moves(df_matrix):\n",
    "    df_zeroed_matrix = df_matrix.copy(deep=True)\n",
    "\n",
    "    np.fill_diagonal(df_zeroed_matrix.values, 0)\n",
    "\n",
    "    # Sum across rows and columns\n",
    "    df_high_loss = df_zeroed_matrix.sum(axis=1).to_frame(name='count')\n",
    "    df_high_gain = df_zeroed_matrix.sum().to_frame(name='count')\n",
    "\n",
    "    # Label fips based on gainers & losers\n",
    "    condList_high_loss = [\n",
    "        df_high_loss.index.isin(se_gainers), df_high_loss.index.isin(se_losers)\n",
    "    ]\n",
    "    condList_high_gain = [\n",
    "        df_high_gain.index.isin(se_gainers), df_high_gain.index.isin(se_losers)\n",
    "    ]\n",
    "    choiceList_high = ['gain', 'loss']\n",
    "\n",
    "    df_high_loss['type'] = pd.Categorical(\n",
    "        np.select(condList_high_loss, choiceList_high, default='other')\n",
    "    )\n",
    "    df_high_gain['type'] = pd.Categorical(\n",
    "        np.select(condList_high_gain, choiceList_high, default='other')\n",
    "    )\n",
    "\n",
    "    return df_high_loss, df_high_gain\n",
    "\n",
    "df_03_07_high_loss, df_03_07_high_gain = high_moves(df_03_07_matrix)\n",
    "df_08_12_high_loss, df_08_12_high_gain = high_moves(df_08_12_matrix)\n",
    "df_13_17_high_loss, df_13_17_high_gain = high_moves(df_13_17_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss.to_csv(\"data/direct/03_07_out_of_high_loss.csv\")\n",
    "df_03_07_high_gain.to_csv(\"data/direct/03_07_into_high_gain.csv\")\n",
    "\n",
    "df_08_12_high_loss.to_csv(\"data/direct/08_12_out_of_high_loss.csv\")\n",
    "df_08_12_high_gain.to_csv(\"data/direct/08_12_into_high_gain.csv\")\n",
    "\n",
    "df_13_17_high_loss.to_csv(\"data/direct/13_17_out_of_high_loss.csv\")\n",
    "df_13_17_high_gain.to_csv(\"data/direct/13_17_into_high_gain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summaries\n",
    "\n",
    "def summary(df_high_moves):\n",
    "    df_summary = pd.DataFrame(columns=['count'])\n",
    "    \n",
    "    # Sum each of gainers, losers, and other\n",
    "    df_summary.loc[\"from_loss\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_losers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_gain\"] = (\n",
    "        df_high_moves[df_high_moves.index.isin(se_gainers)]['count'].sum()\n",
    "    )\n",
    "    df_summary.loc[\"from_other\"] = (\n",
    "        df_high_moves['count'].sum()\n",
    "        - df_summary.loc[\"from_loss\"]\n",
    "        - df_summary.loc[\"from_gain\"]\n",
    "    )\n",
    "\n",
    "    return df_summary\n",
    "\n",
    "df_03_07_high_loss_summary = summary(df_03_07_high_loss)\n",
    "df_03_07_high_gain_summary = summary(df_03_07_high_gain)\n",
    "\n",
    "df_08_12_high_loss_summary = summary(df_08_12_high_loss)\n",
    "df_08_12_high_gain_summary = summary(df_08_12_high_gain)\n",
    "\n",
    "df_13_17_high_loss_summary = summary(df_13_17_high_loss)\n",
    "df_13_17_high_gain_summary = summary(df_13_17_high_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to files\n",
    "df_03_07_high_loss_summary.to_csv(\"data/direct/03_07_out_of_high_loss_summary.csv\")\n",
    "df_03_07_high_gain_summary.to_csv(\"data/direct/03_07_into_high_gain_summary.csv\")\n",
    "\n",
    "df_08_12_high_loss_summary.to_csv(\"data/direct/08_12_out_of_high_loss_summary.csv\")\n",
    "df_08_12_high_gain_summary.to_csv(\"data/direct/08_12_into_high_gain_summary.csv\")\n",
    "\n",
    "df_13_17_high_loss_summary.to_csv(\"data/direct/13_17_out_of_high_loss_summary.csv\")\n",
    "df_13_17_high_gain_summary.to_csv(\"data/direct/13_17_into_high_gain_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infutor-research-jupyterlab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b170992dff1a2d7a7d34a8e195c66b869f5a2c2d367c7625363ea5e606f09977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
