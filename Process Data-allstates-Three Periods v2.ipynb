{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main changes:\n",
    "* Added period 0\n",
    "* originfips=prev non-null fips &rarr; originfips=prev address_observation fips\n",
    "  - Causes different moves to be filtered out\n",
    "* Writes all moves to allmoves.csv\n",
    "* period=0 &rarr; period=''\n",
    "* originfips=\"first record\" &rarr; originfips=-1\n",
    "* prev_effdate=nan &rarr; prev_effdate=-1\n",
    "* Removed fips leading zeros\n",
    "* Removed decimal points\n",
    "\n",
    "Performance:\n",
    "\n",
    "Process Data-allstates-Three Periods.ipynb lowest of three runs: 0.9s\\\n",
    "&darr;\\\n",
    "Process Data-allstates-Three Periods v2.ipynb lowest of three runs: 0.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See what floats can be converted to ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data/allmoves.csv ###\n",
    "\n",
    "# TODO: Use read_csv(chunksize=)/dask\n",
    "\n",
    "# Options\n",
    "z4types = ('S', 'H', 'R')\n",
    "\n",
    "# Settings\n",
    "usecols_all_states = ['pid', 'idate', 'odate', 'z4type', 'effdate']\n",
    "for i in range(2, 11):\n",
    "    usecols_all_states.append(f'z4type{i}')\n",
    "    usecols_all_states.append(f'effdate{i}')\n",
    "for i in range(1, 11):\n",
    "    usecols_all_states.append(f'fips{i}')\n",
    "\n",
    "dtype_all_states = {\n",
    "    'pid': 'object',\n",
    "    'idate': 'float',\n",
    "    'odate': 'float',\n",
    "    'z4type': 'category',\n",
    "    'effdate': 'float',\n",
    "    'fips1': 'float',\n",
    "}\n",
    "for i in range(2, 11):\n",
    "    dtype_all_states[f'z4type{i}'] = 'category'\n",
    "    dtype_all_states[f'effdate{i}'] = 'float'\n",
    "    dtype_all_states[f'fips{i}'] = 'float'\n",
    "\n",
    "na_values_all_states = \"Not in California\"\n",
    "\n",
    "list_dict_col_names = [\n",
    "    {\n",
    "        'pid': 'pid',\n",
    "        'idate': 'idate',\n",
    "        'odate': 'odate',\n",
    "        'z4type': 'z4type',\n",
    "        'effdate': 'effdate',\n",
    "        'fips1': 'fips',\n",
    "        'seentime': 'seentime',\n",
    "    }\n",
    "]\n",
    "for i in range(2, 11):\n",
    "    list_dict_col_names.append(\n",
    "        {\n",
    "            'pid': 'pid',\n",
    "            'idate': 'idate',\n",
    "            'odate': 'odate',\n",
    "            f'z4type{i}': 'z4type',\n",
    "            f'effdate{i}': 'effdate',\n",
    "            f'fips{i}': 'fips',\n",
    "            'seentime': 'seentime',\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Import columns of interest\n",
    "df_all_states = pd.read_csv(\n",
    "    \"data/all_states.csv\", \n",
    "    usecols=usecols_all_states,\n",
    "    dtype=dtype_all_states,\n",
    "    na_values=na_values_all_states\n",
    ")\n",
    "\n",
    "# Calculate seentime\n",
    "df_all_states['seentime'] = df_all_states['odate'] - df_all_states['idate']\n",
    "\n",
    "# Split all_states into individual moves\n",
    "list_df_all_moves = [\n",
    "    df_all_states[list_dict_col_names[i].keys()].rename(\n",
    "        columns=list_dict_col_names[i]\n",
    "    ) for i in range(10)\n",
    "]\n",
    "\n",
    "# Add address_observation column\n",
    "for i in range(len(list_df_all_moves)):\n",
    "    list_df_all_moves[i].insert(0, 'address_observation', i + 1)\n",
    "\n",
    "# Interleave list_df_all_moves\n",
    "df_all_moves = (\n",
    "    pd.concat(list_df_all_moves)\n",
    "        .sort_index(kind='stable')\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Add previous values\n",
    "df_all_moves[['originfips', 'prev_effdate']] = df_all_moves.groupby('pid')[\n",
    "    ['fips', 'effdate']\n",
    "].shift(fill_value=-1)\n",
    "\n",
    "# Filter df_all_moves\n",
    "df_all_moves = df_all_moves.dropna(subset=['fips', 'originfips'])\n",
    "df_all_moves = df_all_moves[df_all_moves['z4type'].isin(z4types)]\n",
    "\n",
    "# Categorize periods\n",
    "condList_period = [\n",
    "    df_all_moves['effdate'].between(201301, 201801, inclusive='left'),\n",
    "    df_all_moves['effdate'].between(200801, 201301, inclusive='left'),\n",
    "    df_all_moves['effdate'].between(200301, 200801, inclusive='left'),\n",
    "]\n",
    "choiceList_period = ['2', '1', '0']\n",
    "df_all_moves.insert(\n",
    "    7,\n",
    "    'period',\n",
    "    pd.Categorical(np.select(condList_period, choiceList_period, default=''))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data/allmoves.csv\n",
    "df_all_moves.to_csv(\"data/allmoves_v2.csv\", float_format=\"%.0f\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load fips gainers and losers ###\n",
    "\n",
    "# Settings\n",
    "usecols_fips_tract = ['tractid_fips', 'gainers', 'losers']\n",
    "\n",
    "dtype_fips_tract = {\n",
    "    'tractid_fips': 'float',\n",
    "    'gainers': 'bool',\n",
    "    'losers': 'bool',\n",
    "}\n",
    "\n",
    "# Import fips_tracts_cats.csv\n",
    "df_fips_tract = pd.read_csv(\n",
    "    \"fips_tracts_cats.csv\",\n",
    "    usecols=usecols_fips_tract,\n",
    "    dtype=dtype_fips_tract\n",
    ")\n",
    "\n",
    "# Get Series of gainers and losers\n",
    "se_gainers = df_fips_tract[df_fips_tract['gainers']]['tractid_fips']\n",
    "se_losers = df_fips_tract[df_fips_tract['losers']]['tractid_fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK begin\n",
    "\n",
    "# Settings\n",
    "arg_all_moves = {\n",
    "    'usecols': ['period', 'fips', 'originfips'],\n",
    "    'dtype': {'period': 'category', 'fips': 'float', 'originfips': 'float'},\n",
    "    'na_values': \"nan\",\n",
    "}\n",
    "\n",
    "# Load allmoves\n",
    "df_all_moves = pd.read_csv(\"data/allmoves_v2.csv\", **arg_all_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract period 0 fips and originfips\n",
    "df_period0 = df_all_moves[df_all_moves['period'] == '0'][['fips', 'originfips']]\n",
    "df_period1 = df_all_moves[df_all_moves['period'] == '1'][['fips', 'originfips']]\n",
    "df_period2 = df_all_moves[df_all_moves['period'] == '2'][['fips', 'originfips']]\n",
    "\n",
    "# end HACK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main changes:\n",
    "* Remove leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create gaintotals and losstotals ###\n",
    "\n",
    "# TODO: Test efficiency\n",
    "\n",
    "# Count fips\n",
    "df_gain_total0 = df_period0.groupby('fips').size().reindex(se_gainers)\n",
    "df_loss_total0 = df_period0.groupby('originfips').size().reindex(se_losers)\n",
    "\n",
    "df_gain_total1 = df_period1.groupby('fips').size().reindex(se_gainers)\n",
    "df_loss_total1 = df_period1.groupby('originfips').size().reindex(se_losers)\n",
    "\n",
    "df_gain_total2 = df_period2.groupby('fips').size().reindex(se_gainers)\n",
    "df_loss_total2 = df_period2.groupby('originfips').size().reindex(se_losers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "arg_to_csv = {\n",
    "    'na_rep': \"0\",\n",
    "    'float_format': \"%.0f\",\n",
    "    'header': False,\n",
    "}\n",
    "\n",
    "# Write output files\n",
    "df_gain_total0.to_csv(\"data/gaintotal_p0.csv\", **arg_to_csv)\n",
    "df_loss_total0.to_csv(\"data/losstotal_p0.csv\", **arg_to_csv)\n",
    "\n",
    "df_gain_total1.to_csv(\"data/gaintotal_p1.csv\", **arg_to_csv)\n",
    "df_loss_total1.to_csv(\"data/losstotal_p1.csv\", **arg_to_csv)\n",
    "\n",
    "df_gain_total2.to_csv(\"data/gaintotal_p2.csv\", **arg_to_csv)\n",
    "df_loss_total2.to_csv(\"data/losstotal_p2.csv\", **arg_to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main changes:\n",
    "* Index order\n",
    "* first record &rarr; -1\n",
    "* Removed leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create matrices ###\n",
    "\n",
    "# Get all fips in each period\n",
    "se_indices0 = pd.unique(\n",
    "    pd.concat([df_period0['originfips'], df_period0['fips']])\n",
    ")\n",
    "\n",
    "se_indices1 = pd.unique(\n",
    "    pd.concat([df_period1['originfips'], df_period1['fips']])\n",
    ")\n",
    "\n",
    "se_indices2 = pd.unique(\n",
    "    pd.concat([df_period2['originfips'], df_period2['fips']])\n",
    ")\n",
    "\n",
    "# Create matrix\n",
    "df_matrix0 = (\n",
    "    df_period0.groupby(['originfips', 'fips'])\n",
    "    .size()\n",
    "    .unstack()\n",
    "    .reindex(se_indices0, columns=se_indices0)\n",
    ")\n",
    "\n",
    "df_matrix1 = (\n",
    "    df_period1.groupby(['originfips', 'fips'])\n",
    "    .size()\n",
    "    .unstack()\n",
    "    .reindex(se_indices1, columns=se_indices1)\n",
    ")\n",
    "\n",
    "df_matrix2 = (\n",
    "    df_period2.groupby(['originfips', 'fips'])\n",
    "    .size()\n",
    "    .unstack()\n",
    "    .reindex(se_indices2, columns=se_indices2)\n",
    ")\n",
    "\n",
    "# Clear top-left cell\n",
    "df_matrix0.index.name = ''\n",
    "\n",
    "df_matrix1.index.name = ''\n",
    "\n",
    "df_matrix2.index.name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write matrix\n",
    "df_matrix0.to_csv(\"data/matrix_p0.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "\n",
    "df_matrix1.to_csv(\"data/matrix_p1.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "\n",
    "df_matrix2.to_csv(\"data/matrix_p2.csv\", na_rep=\"0\", float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/matrix_p2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\Process Data-allstates-Three Periods v2.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Three%20Periods%20v2.ipynb#ch0000013?line=3'>4</a>\u001b[0m arg_read_csv \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Three%20Periods%20v2.ipynb#ch0000013?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Three%20Periods%20v2.ipynb#ch0000013?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Three%20Periods%20v2.ipynb#ch0000013?line=6'>7</a>\u001b[0m }\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Three%20Periods%20v2.ipynb#ch0000013?line=8'>9</a>\u001b[0m df_matrix1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/matrix_p1.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39marg_read_csv)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tedzy/python/repos/infutor-research-jupyterlab/Process%20Data-allstates-Three%20Periods%20v2.ipynb#ch0000013?line=9'>10</a>\u001b[0m df_matrix2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/matrix_p2.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39marg_read_csv)\n",
      "File \u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\tedzy\\python\\repos\\infutor-research-jupyterlab\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/matrix_p2.csv'"
     ]
    }
   ],
   "source": [
    "# HACK begin\n",
    "\n",
    "# Settings\n",
    "arg_read_csv = {\n",
    "    'index_col': 0,\n",
    "    'dtype': 'float',\n",
    "}\n",
    "\n",
    "df_matrix0 = pd.read_csv(\"data/matrix_p0.csv\", **arg_read_csv)\n",
    "df_matrix1 = pd.read_csv(\"data/matrix_p1.csv\", **arg_read_csv)\n",
    "df_matrix2 = pd.read_csv(\"data/matrix_p2.csv\", **arg_read_csv)\n",
    "\n",
    "# end HACK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main changes:\n",
    "* fips='first record' &rarr; fips=-1\n",
    "* Removed leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create summaries ###\n",
    "\n",
    "# FIXME: zero out diagonals of matrix first\n",
    "\n",
    "# Calculate out of high loss\n",
    "se_high_loss0 = df_matrix1[df_matrix0.index.isin(se_losers)].sum(axis=1)\n",
    "\n",
    "se_high_loss1 = df_matrix1[df_matrix1.index.isin(se_losers)].sum(axis=1)\n",
    "\n",
    "se_high_loss2 = df_matrix2[df_matrix2.index.isin(se_losers)].sum(axis=1)\n",
    "\n",
    "# Calculate into high gain\n",
    "# df_matrix0_transpose = df_matrix0.transpose()\n",
    "# df_high_gain0 = df_matrix0_transpose[df_matrix0_transpose.index.isin(se_gainers)].sum(axis=1).to_frame()\n",
    "df_high_gain0 = df_matrix0[df_matrix0.index.isin(se_gainers)].transpose().sum(axis=1).to_frame(name='count')\n",
    "\n",
    "# df_matrix1_transpose = df_matrix1.transpose()\n",
    "# df_high_gain1 = df_matrix1_transpose[df_matrix1_transpose.index.isin(se_gainers)].sum(axis=1).to_frame()\n",
    "df_high_gain1 = df_matrix1[df_matrix1.index.isin(se_gainers)].transpose().sum(axis=1).to_frame(name='count')\n",
    "\n",
    "# df_matrix2_transpose = df_matrix2.transpose()\n",
    "# df_high_gain2 = df_matrix2_transpose[df_matrix2_transpose.index.isin(se_gainers)].sum(axis=1).to_frame()\n",
    "df_high_gain2 = df_matrix2[df_matrix2.index.isin(se_gainers)].transpose().sum(axis=1).to_frame(name='count')\n",
    "\n",
    "# Remove index name\n",
    "df_high_gain0.index.name = ''\n",
    "\n",
    "df_high_gain1.index.name = ''\n",
    "\n",
    "df_high_gain2.index.name = ''\n",
    "\n",
    "# Categorize gain type\n",
    "choiceList_high_gain = ['gain', 'loss']\n",
    "\n",
    "condList_high_gain0 = [df_high_gain1.index.isin(se_gainers), df_high_gain1.index.isin(se_losers)]\n",
    "df_high_gain0['type'] = pd.Categorical(np.select(condList_high_gain0, choiceList_high_gain, default='other'))\n",
    "\n",
    "condList_high_gain1 = [df_high_gain1.index.isin(se_gainers), df_high_gain1.index.isin(se_losers)]\n",
    "df_high_gain1['type'] = pd.Categorical(np.select(condList_high_gain1, choiceList_high_gain, default='other'))\n",
    "\n",
    "condList_high_gain2 = [df_high_gain2.index.isin(se_gainers), df_high_gain2.index.isin(se_losers)]\n",
    "df_high_gain2['type'] = pd.Categorical(np.select(condList_high_gain2, choiceList_high_gain, default='other'))\n",
    "\n",
    "# Calculate summary\n",
    "from_loss0 = df_high_gain0[df_high_gain0.index.isin(se_losers)]['count'].sum()\n",
    "from_gain0 = df_high_gain0[df_high_gain0.index.isin(se_gainers)]['count'].sum()\n",
    "from_other0 = df_high_gain0['count'].sum() - from_loss0 - from_gain0\n",
    "\n",
    "from_loss1 = df_high_gain1[df_high_gain1.index.isin(se_losers)]['count'].sum()\n",
    "from_gain1 = df_high_gain1[df_high_gain1.index.isin(se_gainers)]['count'].sum()\n",
    "from_other1 = df_high_gain1['count'].sum() - from_loss1 - from_gain1\n",
    "\n",
    "from_loss2 = df_high_gain2[df_high_gain2.index.isin(se_losers)]['count'].sum()\n",
    "from_gain2 = df_high_gain2[df_high_gain2.index.isin(se_gainers)]['count'].sum()\n",
    "from_other2 = df_high_gain2['count'].sum() - from_loss2 - from_gain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output files\n",
    "se_high_loss0.to_csv(\"data/out_of_high_loss_p0.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "df_high_gain0.to_csv(\"data/into_high_gain_p0.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "with open(\"data/into_high_gain_summary_p0.csv\", 'w') as f:\n",
    "    f.write(f\"from_loss,{int(from_loss0)}\\n\")\n",
    "    f.write(f\"from_gain,{int(from_gain0)}\\n\")\n",
    "    f.write(f\"from_other,{int(from_other0)}\\n\")\n",
    "\n",
    "se_high_loss1.to_csv(\"data/out_of_high_loss_p1.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "df_high_gain1.to_csv(\"data/into_high_gain_p1.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "with open(\"data/into_high_gain_summary_p1.csv\", 'w') as f:\n",
    "    f.write(f\"from_loss,{int(from_loss1)}\\n\")\n",
    "    f.write(f\"from_gain,{int(from_gain1)}\\n\")\n",
    "    f.write(f\"from_other,{int(from_other1)}\\n\")\n",
    "\n",
    "se_high_loss2.to_csv(\"data/out_of_high_loss_p2.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "df_high_gain2.to_csv(\"data/into_high_gain_p2.csv\", na_rep=\"0\", float_format=\"%.0f\")\n",
    "with open(\"data/into_high_gain_summary_p2.csv\", 'w') as f:\n",
    "    f.write(f\"from_loss,{int(from_loss2)}\\n\")\n",
    "    f.write(f\"from_gain,{int(from_gain2)}\\n\")\n",
    "    f.write(f\"from_other,{int(from_other2)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infutor-research-jupyterlab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b170992dff1a2d7a7d34a8e195c66b869f5a2c2d367c7625363ea5e606f09977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
