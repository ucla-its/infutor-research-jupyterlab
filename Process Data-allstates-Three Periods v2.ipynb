{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main changes:\n",
    "* Added period 0\n",
    "* originfips=prev non-null fips &rarr; originfips=prev address_observation fips\n",
    "  - Causes different moves to be filtered out\n",
    "* Writes all moves to allmoves.csv\n",
    "* period=0 &rarr; period=''\n",
    "* originfips=\"first record\" &rarr; originfips=-1\n",
    "* prev_effdate=nan &rarr; prev_effdate=-1\n",
    "* Removed fips leading zeros\n",
    "* Removed decimal points\n",
    "\n",
    "Performance:\n",
    "\n",
    "Process Data-allstates-Three Periods.ipynb lowest of three runs: 0.9s\\\n",
    "&darr;\\\n",
    "Process Data-allstates-Three Periods v2.ipynb lowest of three runs: 0.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data/allmoves.csv ###\n",
    "\n",
    "# TODO: Use read_csv(chunksize=)/dask\n",
    "\n",
    "# Options\n",
    "z4types = ('S', 'H', 'R')\n",
    "\n",
    "# Settings\n",
    "usecols_all_states = ['pid', 'idate', 'odate', 'z4type', 'effdate']\n",
    "for i in range(2, 11):\n",
    "    usecols_all_states.append(f'z4type{i}')\n",
    "    usecols_all_states.append(f'effdate{i}')\n",
    "for i in range(1, 11):\n",
    "    usecols_all_states.append(f'fips{i}')\n",
    "\n",
    "dtype_all_states = {\n",
    "    'pid': 'object',\n",
    "    'idate': 'float',\n",
    "    'odate': 'float',\n",
    "    'z4type': 'category',\n",
    "    'effdate': 'float',\n",
    "    'fips1': 'float',\n",
    "}\n",
    "for i in range(2, 11):\n",
    "    dtype_all_states[f'z4type{i}'] = 'category'\n",
    "    dtype_all_states[f'effdate{i}'] = 'float'\n",
    "    dtype_all_states[f'fips{i}'] = 'float'\n",
    "\n",
    "na_values_all_states = \"Not in California\"\n",
    "\n",
    "list_dict_col_names = [\n",
    "    {\n",
    "        'pid': 'pid',\n",
    "        'idate': 'idate',\n",
    "        'odate': 'odate',\n",
    "        'z4type': 'z4type',\n",
    "        'effdate': 'effdate',\n",
    "        'fips1': 'fips',\n",
    "        'seentime': 'seentime',\n",
    "    }\n",
    "]\n",
    "for i in range(2, 11):\n",
    "    list_dict_col_names.append(\n",
    "        {\n",
    "            'pid': 'pid',\n",
    "            'idate': 'idate',\n",
    "            'odate': 'odate',\n",
    "            f'z4type{i}': 'z4type',\n",
    "            f'effdate{i}': 'effdate',\n",
    "            f'fips{i}': 'fips',\n",
    "            'seentime': 'seentime',\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Import columns of interest\n",
    "df_all_states = pd.read_csv(\n",
    "    \"data/all_states.csv\", \n",
    "    usecols=usecols_all_states,\n",
    "    dtype=dtype_all_states,\n",
    "    na_values=na_values_all_states\n",
    ")\n",
    "\n",
    "# Calculate seentime\n",
    "df_all_states['seentime'] = df_all_states['odate'] - df_all_states['idate']\n",
    "\n",
    "# Split all_states into individual moves\n",
    "list_df_all_moves = [\n",
    "    df_all_states[list_dict_col_names[i].keys()].rename(\n",
    "        columns=list_dict_col_names[i]\n",
    "    ) for i in range(10)\n",
    "]\n",
    "\n",
    "# Add address_observation column\n",
    "for i in range(len(list_df_all_moves)):\n",
    "    list_df_all_moves[i].insert(0, 'address_observation', i + 1)\n",
    "\n",
    "# Interleave list_df_all_moves\n",
    "df_all_moves = (\n",
    "    pd.concat(list_df_all_moves)\n",
    "        .sort_index(kind='stable')\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Add previous values\n",
    "df_all_moves[['originfips', 'prev_effdate']] = df_all_moves.groupby('pid')[\n",
    "    ['fips', 'effdate']\n",
    "].shift(fill_value=-1)\n",
    "\n",
    "# Filter df_all_moves\n",
    "df_all_moves = df_all_moves.dropna(subset=['fips', 'originfips'])\n",
    "df_all_moves = df_all_moves[df_all_moves['z4type'].isin(z4types)]\n",
    "\n",
    "# Categorize periods\n",
    "condList_period = [\n",
    "    df_all_moves['effdate'].between(201301, 201801, inclusive='left'),\n",
    "    df_all_moves['effdate'].between(200801, 201301, inclusive='left'),\n",
    "    df_all_moves['effdate'].between(200301, 200801, inclusive='left'),\n",
    "]\n",
    "choiceList_period = ['2', '1', '0']\n",
    "df_all_moves.insert(\n",
    "    7,\n",
    "    'period',\n",
    "    pd.Categorical(np.select(condList_period, choiceList_period, default=''))\n",
    ")\n",
    "\n",
    "# Write data/allmoves.csv\n",
    "df_all_moves.to_csv(\"data/allmoves_v2.csv\", float_format=\"%.0f\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load fips gainers and losers ###\n",
    "\n",
    "# Settings\n",
    "usecols_fips_tract = ['tractid_fips', 'gainers', 'losers']\n",
    "\n",
    "dtype_fips_tract = {\n",
    "    'tractid_fips': 'float',\n",
    "    'gainers': 'bool',\n",
    "    'losers': 'bool',\n",
    "}\n",
    "\n",
    "# Import fips_tracts_cats.csv\n",
    "df_fips_tract = pd.read_csv(\n",
    "    \"fips_tracts_cats.csv\",\n",
    "    usecols=usecols_fips_tract,\n",
    "    dtype=dtype_fips_tract\n",
    ")\n",
    "\n",
    "# Get Series of gainers and losers\n",
    "se_gainers = df_fips_tract[df_fips_tract['gainers']]['tractid_fips']\n",
    "se_losers = df_fips_tract[df_fips_tract['losers']]['tractid_fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK begin\n",
    "\n",
    "# Settings\n",
    "arg_all_moves = {\n",
    "    'usecols': ['period', 'fips', 'originfips'],\n",
    "    'dtype': {'period': 'category', 'fips': 'float', 'originfips': 'float'},\n",
    "    'na_values': \"nan\",\n",
    "}\n",
    "\n",
    "# Load allmoves\n",
    "df_all_moves = pd.read_csv(\"data/allmoves_v2.csv\", **arg_all_moves)\n",
    "\n",
    "# Extract period 0 fips and originfips\n",
    "df_period0 = df_all_moves[df_all_moves['period'] == '0'][['fips', 'originfips']]\n",
    "df_period1 = df_all_moves[df_all_moves['period'] == '1'][['fips', 'originfips']]\n",
    "df_period2 = df_all_moves[df_all_moves['period'] == '2'][['fips', 'originfips']]\n",
    "\n",
    "# end HACK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main changes:\n",
    "* Remove leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test efficiency\n",
    "\n",
    "# Settings\n",
    "arg_to_csv = {\n",
    "    'na_rep': \"0\",\n",
    "    'float_format': \"%.0f\",\n",
    "    'header': False,\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "# Count fips\n",
    "df_gain_total0 = pd.merge(\n",
    "    se_gainers.rename('fips'),\n",
    "    df_period0.groupby('fips', as_index=False).size(),\n",
    "    how='left'\n",
    ")\n",
    "df_loss_total0 = pd.merge(\n",
    "    se_losers.rename('originfips'),\n",
    "    df_period0.groupby('originfips', as_index=False).size(),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_gain_total1 = pd.merge(\n",
    "    se_gainers.rename('fips'),\n",
    "    df_period1.groupby('fips', as_index=False).size(),\n",
    "    how='left'\n",
    ")\n",
    "df_loss_total1 = pd.merge(\n",
    "    se_losers.rename('originfips'),\n",
    "    df_period1.groupby('originfips', as_index=False).size(),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_gain_total2 = pd.merge(\n",
    "    se_gainers.rename('fips'),\n",
    "    df_period2.groupby('fips', as_index=False).size(),\n",
    "    how='left'\n",
    ")\n",
    "df_loss_total2 = pd.merge(\n",
    "    se_losers.rename('originfips'),\n",
    "    df_period2.groupby('originfips', as_index=False).size(),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Write output files\n",
    "df_gain_total0.to_csv(\"data/gaintotal_p0.csv\", **arg_to_csv)\n",
    "df_loss_total0.to_csv(\"data/losstotal_p0.csv\", **arg_to_csv)\n",
    "\n",
    "df_gain_total1.to_csv(\"data/gaintotal_p1.csv\", **arg_to_csv)\n",
    "df_loss_total1.to_csv(\"data/losstotal_p1.csv\", **arg_to_csv)\n",
    "\n",
    "df_gain_total2.to_csv(\"data/gaintotal_p2.csv\", **arg_to_csv)\n",
    "df_loss_total2.to_csv(\"data/losstotal_p2.csv\", **arg_to_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infutor-research-jupyterlab': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b170992dff1a2d7a7d34a8e195c66b869f5a2c2d367c7625363ea5e606f09977"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
