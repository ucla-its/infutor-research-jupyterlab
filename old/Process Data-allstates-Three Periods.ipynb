{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_assign(row):\n",
    "    if row[\"effdate\"] >= 201301 and row[\"effdate\"] < 201801:\n",
    "        val = 2\n",
    "    elif row[\"effdate\"] >= 200801 and row[\"effdate\"] < 201301:\n",
    "        val = 1\n",
    "    elif row[\"effdate\"] >= 200301 and row[\"effdate\"] < 200801:\n",
    "        val = 0\n",
    "    else:\n",
    "        val = 99\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: 100, PID: Y39398460938264 at 13:54:38.211734 | rate: 7104.7 per min. | this batch complete: 3857.9 min\n"
     ]
    }
   ],
   "source": [
    "z4types = [\"S\", \"H\", \"R\"]\n",
    "long_moves = pd.DataFrame()\n",
    "\n",
    "status_file = \"status_allmoves.txt\"\n",
    "source_file = \"data/all_states.csv\"\n",
    "\n",
    "print_line_every = 100\n",
    "start_line = 1\n",
    "process_records = 27408937\n",
    "end_line = start_line + process_records\n",
    "\n",
    "with open(status_file) as f:\n",
    "    last_status_line = int(f.read())\n",
    "# resume_from_status = input(\n",
    "#     f\"Type Yes to resume from last line processed in {status_file} which is: \"\n",
    "#     f\"{last_status_line:,}\\n\"\n",
    "# )\n",
    "resume_from_status = \"No\"\n",
    "if resume_from_status == \"Yes\":\n",
    "    start_line = last_status_line + 1\n",
    "\n",
    "with open(source_file, \"r\") as object:\n",
    "    line_count = start_line\n",
    "    process_start = datetime.now()\n",
    "    csv_file = csv.reader(object)\n",
    "\n",
    "    for row in csv_file:\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"address_observation\": [x + 1 for x in range(10)],\n",
    "                \"pid\": [row[0] for _ in range(10)],\n",
    "                \"idate\": [row[4] for _ in range(10)],\n",
    "                \"odate\": [row[5] for _ in range(10)],\n",
    "                \"z4type\": [row[9 * x + 13] for x in range(10)],\n",
    "                \"effdate\": [row[9 * x + 14] for x in range(10)],\n",
    "                \"fips\": [row[4 * x + 99] for x in range(10)],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df = df[df.fips != \"\"]\n",
    "        df = df[df[\"z4type\"].isin(z4types)]\n",
    "\n",
    "        df[\"effdate\"] = pd.to_numeric(df[\"effdate\"])\n",
    "        df[\"idate\"] = pd.to_numeric(df[\"idate\"])\n",
    "        df[\"odate\"] = pd.to_numeric(df[\"odate\"])\n",
    "        df[\"period\"] = \"\"\n",
    "        df[\"period\"] = df.apply(period_assign, axis=1, result_type=\"reduce\")\n",
    "        df[\"seentime\"] = abs(df[\"odate\"] - df[\"idate\"])\n",
    "\n",
    "        # TODO: Remove PO box addresses\n",
    "\n",
    "        df.sort_values(by=[\"effdate\"], ascending=True, inplace=True)\n",
    "        df[\"originfips\"] = df.fips.shift(1, fill_value=\"first record\")\n",
    "        df[\"prev_effdate\"] = df.effdate.shift(1).astype(str)\n",
    "\n",
    "        df.dropna(axis=0, subset=[\"originfips\"], how=\"any\", inplace=True)\n",
    "        long_moves = pd.concat([long_moves, df])\n",
    "\n",
    "        line_count += 1\n",
    "        if line_count % print_line_every == 0:\n",
    "            long_moves.to_csv(\n",
    "                path_or_buf=\"data/allmoves_v1.csv\", mode=\"a\", header=False, index=False\n",
    "            )\n",
    "            long_moves.drop(long_moves.index, inplace=True)\n",
    "\n",
    "            process_duration = datetime.now() - process_start\n",
    "            rate = print_line_every / (process_duration.total_seconds() / 60)\n",
    "            estimated_completion = (end_line - line_count) / rate\n",
    "            print(\n",
    "                f\"Finished: {line_count}, PID: {row[0]} at \"\n",
    "                f\"{datetime.now().time()} | rate: {rate:.1f} per min. | this \"\n",
    "                f\"batch complete: {estimated_completion:.1f} min\"\n",
    "            )\n",
    "\n",
    "            process_start = datetime.now()\n",
    "\n",
    "            with open(status_file, \"w\") as writer:\n",
    "                writer.write(str(line_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: convert data/allmoves.csv to data/method2/allmoves_w_header.csv\n",
    "df = pd.read_csv(\"data/method2/allmoves_w_header.csv\")\n",
    "# TODO: Keep all moves in periods 1, 2, 3 that are also into/out of LA or Orange County\n",
    "df.drop_duplicates(subset=\"pid\", inplace=True)\n",
    "df.to_csv(\"data/method2/allmoves_deduped.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_dict = {}\n",
    "\n",
    "with open(\"fips_tracts_cats.csv\", \"r\") as object:\n",
    "    csv_reader = csv.reader(object)\n",
    "    for row in csv_reader:\n",
    "        k = row[1]\n",
    "        if row[5] == \"1\":\n",
    "            v = \"gain\"\n",
    "        elif row[6] == \"1\":\n",
    "            v = \"loss\"\n",
    "        else:\n",
    "            v = \"other\"\n",
    "        fips_dict[k] = v\n",
    "\n",
    "gain_list = [k for k, v in fips_dict.items() if v == \"gain\"]\n",
    "loss_list = [k for k, v in fips_dict.items() if v == \"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/method2/allmoves_deduped.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_dict = {}\n",
    "\n",
    "for i in gain_list:\n",
    "    df1 = df[df.fips == i]\n",
    "    df2 = df1[df.period == 0]\n",
    "    gain_dict[i] = len(df2)\n",
    "\n",
    "print(\"Loss List Time\")\n",
    "loss_dict = {}\n",
    "\n",
    "for i in loss_list:\n",
    "    df1 = df[df.originfips == i]\n",
    "    df2 = df1[df.period == 0]\n",
    "    loss_dict[i] = len(df2)\n",
    "\n",
    "\n",
    "# XXX: output file\n",
    "with open(\"data/losstotal_p0.csv\", \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(loss_dict.items())\n",
    "\n",
    "# XXX: output file\n",
    "with open(\"data/gaintotal_p0.csv\", \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(gain_dict.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.period == 0]\n",
    "\n",
    "origins = df1.originfips.unique().tolist()\n",
    "destinations = df1.fips.unique().tolist()\n",
    "\n",
    "focus_fips = loss_list + gain_list\n",
    "all_fips = origins + destinations\n",
    "\n",
    "my_dict = {i: all_fips.count(i) for i in all_fips}\n",
    "unique_fips = list(my_dict.keys())\n",
    "\n",
    "\n",
    "matrix = pd.DataFrame(0, index=unique_fips, columns=unique_fips)\n",
    "\n",
    "for index, row in df1.iterrows():\n",
    "    origin = row[\"originfips\"]\n",
    "    destination = row[\"fips\"]\n",
    "    matrix.loc[origin, destination] += 1\n",
    "\n",
    "# XXX: output file\n",
    "matrix.to_csv(\"data/matrix_p0.csv\")\n",
    "print(matrix.shape)\n",
    "print(np.count_nonzero(matrix.index.duplicated(\"first\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_csv(\"data/matrix_p0.csv\", dtype=str, index_col=0, header=0)\n",
    "\n",
    "for i in unique_fips:\n",
    "    matrix[i] = pd.to_numeric(matrix[i], errors=\"ignore\")\n",
    "\n",
    "df1 = matrix[matrix.index.isin(loss_list)]\n",
    "\n",
    "for index, row in df1.iterrows():\n",
    "    df1.loc[df1.index, df1.index] = 0\n",
    "\n",
    "df1totals = df1.sum(axis=1).astype(int)\n",
    "# XXX: output file\n",
    "df1totals.to_csv(\"data/out_of_high_loss_p0.csv\")\n",
    "\n",
    "\n",
    "df2 = matrix[matrix.index.isin(gain_list)].transpose()\n",
    "\n",
    "for index, row in df1.iterrows():\n",
    "    df2.loc[index, index] = 0\n",
    "\n",
    "df2totals = df2.sum(axis=1).astype(int)\n",
    "\n",
    "df2totals = df2totals.to_frame()\n",
    "df2totals.rename(columns={0: \"count\"}, inplace=True)\n",
    "df2totals[\"type\"] = \"other\"\n",
    "\n",
    "for index, row in df2totals.iterrows():\n",
    "    df2totals.at[index, \"type\"] = fips_dict.get(index)\n",
    "\n",
    "# XXX: output file\n",
    "df2totals.to_csv(\"data/into_high_gain_p0.csv\")\n",
    "\n",
    "\n",
    "from_loss = np.sum(df2totals[df2totals.index.isin(loss_list)][\"count\"])\n",
    "from_gain = np.sum(df2totals[df2totals.index.isin(gain_list)][\"count\"])\n",
    "from_other = np.sum(df2totals[\"count\"]) - from_loss - from_gain\n",
    "\n",
    "dict_counts = {\"from_loss\": from_loss, \"from_gain\": from_gain, \"from_other\": from_other}\n",
    "# XXX: output file\n",
    "with open(\"data/into_high_gain_summary_p0.csv\", \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(dict_counts.items())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mathy-stuff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db87f8a37618c39d38f3440f4ed8a1840c0cace07e2de00ae4d457b7355f444f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
